from __future__ import annotations
import asyncio
import json
import typing as tp

from celery import Celery

from dishka import AsyncContainer, make_async_container

from app.common.storages.interfaces import KeyValueStorageProtocol
from app.infrastructure.adapters.interfaces import ILLMQueue
from app.common.logger import AISearchLogger, LoggerType
from app.settings.config import settings, Settings, HybridSearchSettings

container: AsyncContainer | None = None
logger: AISearchLogger | None = None

# ---- Celery client (–ª—ë–≥–∫–∏–π) ----
celery_client = Celery(
    "queue-pump",
    broker=str(settings.redis.dsn),      # redis://.../7  ‚Üê –±—Ä–æ–∫–µ—Ä Celery
    backend=str(settings.redis.dsn),     # –º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ db 8
)
celery_client.conf.update(
    task_create_missing_queues=True,
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
)

def init_container() -> AsyncContainer:
    global container, logger
    from app.infrastructure.ioc import ApplicationProvider
    from app.infrastructure.providers import LoggerProvider, MilvusProvider, RedisProvider
    from app.settings.config import (
        AppSettings,
        LLMGlobalSemaphoreSettings,
        LLMQueueSettings,
        MilvusSettings,
        RedisSettings,
        VLLMSettings,
    )

    container = make_async_container(
        ApplicationProvider(),
        LoggerProvider(),
        MilvusProvider(),
        RedisProvider(),
        context={
            AppSettings: settings.app,
            Settings: settings,
            MilvusSettings: settings.milvus,
            RedisSettings: settings.redis,
            LoggerType: LoggerType.CELERY,
            HybridSearchSettings: settings.hybrid,
            LLMGlobalSemaphoreSettings: settings.llm_global_sem,
            LLMQueueSettings: settings.llm_queue,
            VLLMSettings: settings.vllm,
        },
    )
    logger = AISearchLogger(logger_type=LoggerType.CELERY)
    return container

async def _queue_drain_loop() -> None:
    """–§–æ–Ω–æ–≤–∞—è –∫–æ—Ä—É—Ç–∏–Ω–∞: –∂–¥—ë—Ç —Ç–∏–∫–µ—Ç—ã –∏ —à–ª—ë—Ç –∏—Ö –≤ Celery"""
    assert container is not None
    queue = await container.get(ILLMQueue)
    redis_storage = await container.get(KeyValueStorageProtocol)
    logger = await container.get(AISearchLogger)
    logger.info("üö© –°—Ç–∞—Ä—Ç –∫–æ—Ä—É—Ç–∏–Ω—ã —Ä–∞–±–æ—Ç—ã —Å –æ—á–µ—Ä–µ–¥—å—é")
    while True:
        item = await queue.dequeue_blocking(timeout=1)
        if not item:
            await asyncio.sleep(0.1)
            continue

        ticket_id, payload = item
        if not isinstance(payload, dict) or "pack_key" not in payload or "result_key" not in payload:
            logger.error(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ–ª–µ–∑–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–æ–π —Ç–∏–∫–µ—Ç–∞ {ticket_id}, payload: {payload}")
            await queue.set_failed(ticket_id, "‚ö†Ô∏è Invalid payload")
            await queue.ack(ticket_id)
            continue

        pack_key = payload["pack_key"]
        raw = await redis_storage.get(pack_key)
        if raw is None:
            await queue.set_failed(ticket_id, "‚ö†Ô∏è Missing pack")
            await queue.ack(ticket_id)
            continue

        try:
            task_type = json.loads(raw).get("type")
            if task_type == "search":
                logger.info("üö© –ù–ê–ß–ê–õ–û –í–´–ü–û–õ–ù–ï–ù–ò–Ø –ó–ê–î–ê–ß–ò —Ç–∏–ø–∞ 'search'")
                celery_client.send_task(
                    "search_task",
                    args=(ticket_id, payload["pack_key"], payload["result_key"]),
                    queue="gpu-search",
                    task_id=ticket_id  # —É–¥–æ–±–Ω–æ —Å–≤—è–∑—ã–≤–∞—Ç—å —Ç–∏–∫–µ—Ç –∏ task_id
                )

            elif task_type == "generate":
                logger.info("üö© –ù–ê–ß–ê–õ–û –í–´–ü–û–õ–ù–ï–ù–ò–Ø –ó–ê–î–ê–ß–ò —Ç–∏–ø–∞ 'generate'")
                celery_client.send_task(
                    "generate-answer-vllm",
                    args=(ticket_id, payload["pack_key"], payload["result_key"]),
                    queue="gpu-search",
                    task_id=ticket_id
                )
            else:
                logger.info(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∑–∞–¥–∞—á–∏: {task_type}")
                await queue.set_failed(ticket_id, f"unknown task type: {task_type}")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è –û–®–ò–ë–ö–ê: {e}")
            await queue.set_failed(ticket_id, f"‚ö†Ô∏è send_task error: {e}")
        finally:
            logger.info(f"üßπ –£–ë–ò–†–ê–ï–ú –¢–ò–ö–ï–¢ –ò–ó PROCESSING {ticket_id}")
            await queue.ack(ticket_id)

async def _processing_sweeper(period_sec: int = 10, stale_sec: int = 60) -> None:
    """–ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ç—É—Ö—à–∏–µ —Ç–∏–∫–µ—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å"""
    assert container is not None
    queue = await container.get(ILLMQueue)
    logger = await container.get(AISearchLogger)
    logger.info("üö© –°—Ç–∞—Ä—Ç –∫–æ—Ä—É—Ç–∏–Ω—ã –æ—á–∏—Å—Ç–∫–∏ –∑–∞—Å—Ç—Ä—è–≤—à–∏—Ö —Ç–∏–∫–µ—Ç–æ–≤")
    while True:
        try:
            n = await queue.sweep_processing(stale_sec=stale_sec)
            if n:
                logger.warning(f"üö® –ö–æ—Ä—É—Ç–∏–Ω–∞ sweep_processing: –ø–µ—Ä–µ—Å—Ç–∞–≤–∏–ª–∞ {n} –ø–æ–¥–≤–∏—Å—à–∏—Ö —Ç–∏–∫–µ—Ç–æ–≤")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è sweep_processing error: {e}")
        await asyncio.sleep(period_sec)

async def main_async() -> None:
    init_container()
    drain_task = asyncio.create_task(_queue_drain_loop())
    sweeper_task = asyncio.create_task(_processing_sweeper())
    logger = await container.get(AISearchLogger)

    try:
        await asyncio.gather(drain_task, sweeper_task)
    except asyncio.CancelledError as e:
        logger.error(f"‚ö†Ô∏è –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—É—â–µ–Ω–Ω—ã–µ –∫–æ—Ä—É—Ç–∏–Ω—ã: {e}")
        drain_task.cancel()
        sweeper_task.cancel()
        await asyncio.gather(drain_task, sweeper_task, return_exceptions=True)

if __name__ == "__main__":
    asyncio.run(main_async())
