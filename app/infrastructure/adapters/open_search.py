import json
import time
import traceback
import typing as tp

from opensearchpy import (
    OpenSearch,
    helpers as os_helpers,
)

from app.common.logger import AISearchLogger
from app.domain.schemas.open_search import OSIndexSchema
from app.infrastructure.adapters.interfaces import IOpenSearchAdapter
from app.infrastructure.utils.metrics import metrics_print
from app.settings.config import Settings


class OpenSearchAdapter(IOpenSearchAdapter):
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è opensearch"""

    def __init__(self, settings: Settings, logger: AISearchLogger):
        os_init_start = time.perf_counter()
        self.config = settings.opensearch
        self.client = OpenSearch(
            hosts=[{"host": self.config.host, "port": self.config.port}],
            http_compress=True,
            http_auth=(
                (self.config.user, self.config.password) if self.config.user else None
            ),
            use_ssl=self.config.use_ssl,
            verify_certs=self.config.verify_certs,
        )
        self.logger = logger
        self.os_schema = self._load_os_schema_from_json(self.config.schema_path)
        metrics_print("üïí –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OPENSEARCH", os_init_start)

    def build_index(self, data: dict[str, tp.Any]) -> None:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        self.logger.info("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ OS ...")
        self._ensure_os_index(recreate=self.config.recreate_index)
        self._os_optimize_for_bulk(on=True)
        self._os_bulk_index(data=data, chunk_size=self.config.bulk_chunk_size)
        self._os_optimize_for_bulk(on=False)

        self.logger.info("–ò–Ω–¥–µ–∫—Å OS –ø–æ—Å—Ç—Ä–æ–µ–Ω")

    def search(self, body: dict, size: int) -> list[dict]:
        """–ü–æ–∏—Å–∫ –ø—Ä–∏ –ø–æ–º–æ—â–∏ opensearch"""
        resp = self.client.search(
            index=self.config.index_name, body=body, size=size
        )  # –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ self.os_schema.index_name - –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ recreate –±—É–¥–µ—Ç —Å—Ç–æ—è—Ç—å –≤—Å–µ–≥–¥–∞, —á—Ç–æ –Ω–µ –æ—á —Ö–æ—Ä–æ—à–æ
        return resp["hits"]["hits"]

    def _os_optimize_for_bulk(self, on: bool) -> None:
        if on:
            self.client.indices.put_settings(
                index=self.config.index_name,
                body={
                    "index": {"refresh_interval": "-1", "translog.durability": "async"}
                },
            )
        else:
            self.client.indices.put_settings(
                index=self.config.index_name,
                body={
                    "index": {
                        "refresh_interval": "1s",
                        "translog.durability": "request",
                    }
                },
            )
            self.client.indices.refresh(index=self.config.index_name)

    def _os_bulk_index(
        self, data: list[dict[str, tp.Any]], chunk_size: int = 1000
    ) -> None:
        chunk_size = chunk_size or self.os_schema.bulk_chunk_size
        idx_name = self.os_schema.index_name
        id_field = self.os_schema.id_field

        # –ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∫ —Ç–∏–ø–∞–º –∏–∑ –º—ç–ø–ø–∏–Ω–≥–∞
        props = self.os_schema.mappings.get("properties") or {}
        type_of: dict[str, str] = {k: (v.get("type") or "") for k, v in props.items()}

        def coerce(field: str, value: tp.Any) -> tp.Any:
            t = type_of.get(field, "")
            if value is None:
                return None
            try:
                if t in ("keyword", "text"):
                    return str(value)
                if t in ("integer", "short", "byte", "long"):
                    return int(value)
                if t in ("float", "half_float", "scaled_float", "double"):
                    return float(value)
                if t == "boolean":
                    return bool(value)
                return value
            except Exception:
                return None

        def gen_actions() -> tp.Iterator[dict[str, tp.Any]]:
            for row in data:
                # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—è –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç–∏–ø–∞–º–∏
                doc = {
                    field: coerce(field, value)
                    for field, value in row.items()
                    if field in type_of
                }

                # –§–æ—Ä–º–∏—Ä—É–µ–º _id –Ω–∞ –æ—Å–Ω–æ–≤–µ id_field
                _id = None
                if isinstance(id_field, list):
                    # –°–æ—Å—Ç–∞–≤–Ω–æ–π ID: –±–µ—Ä–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª–µ–π –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —á–µ—Ä–µ–∑ "::"
                    id_parts = []
                    for field_name in id_field:
                        value = doc.get(field_name)
                        if value is not None:
                            id_parts.append(str(value))
                    if id_parts:
                        _id = "::".join(id_parts)
                else:
                    # –ü—Ä–æ—Å—Ç–æ–π ID: –±–µ—Ä–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –ø–æ–ª—è
                    _id = doc.get(id_field)

                yield {
                    "_op_type": "index",
                    "_index": idx_name,
                    **({"_id": _id} if _id is not None else {}),
                    "_source": doc,
                }

        os_helpers.bulk(
            self.client, gen_actions(), chunk_size=chunk_size, request_timeout=120
        )

    def upsert(self, data: list[dict[str, tp.Any]]) -> None:
        """Upsert –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ OpenSearch (–æ–±–Ω–æ–≤–ª—è–µ—Ç –∏–ª–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç)"""
        if not data:
            self.logger.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è upsert –≤ OS")
            return

        self.logger.info(f"üîÑ Upsert {len(data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ OS ...")

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∏–Ω–¥–µ–∫—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self._ensure_os_index(recreate=False)

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–¥ bulk-–∑–∞–ø–∏—Å—å—é
        self._os_optimize_for_bulk(on=True)
        self._os_bulk_upsert(data=data, chunk_size=self.config.bulk_chunk_size)
        self._os_optimize_for_bulk(on=False)

        self.logger.info("‚úÖ Upsert OS –∑–∞–≤–µ—Ä—à–µ–Ω")

    def _os_bulk_upsert(
        self, data: list[dict[str, tp.Any]], chunk_size: int = 1000
    ) -> None:
        idx_name = self.os_schema.index_name
        id_field = self.os_schema.id_field
        props = self.os_schema.mappings.get("properties", {})
        type_of: dict[str, str] = {k: (v.get("type") or "") for k, v in props.items()}

        def coerce(field: str, value: tp.Any) -> tp.Any:
            t = type_of.get(field, "")
            if value is None:
                return None
            try:
                if t in ("keyword", "text"):
                    return str(value)
                if t in ("integer", "short", "byte", "long"):
                    return int(value)
                if t in ("float", "half_float", "scaled_float", "double"):
                    return float(value)
                if t == "boolean":
                    return bool(value)
                return value
            except Exception:
                return None

        def gen_actions() -> tp.Iterator[dict[str, tp.Any]]:
            for row in data:
                doc = {
                    field: coerce(field, value)
                    for field, value in row.items()
                    if field in type_of
                }

                # –§–æ—Ä–º–∏—Ä—É–µ–º _id –Ω–∞ –æ—Å–Ω–æ–≤–µ id_field
                _id = None
                if isinstance(id_field, list):
                    # –°–æ—Å—Ç–∞–≤–Ω–æ–π ID
                    id_parts = []
                    for field_name in id_field:
                        value = doc.get(field_name)
                        if value is not None:
                            id_parts.append(str(value))
                    if id_parts:
                        _id = "::".join(id_parts)
                else:
                    # –ü—Ä–æ—Å—Ç–æ–π ID
                    _id = doc.get(id_field)

                if _id is None:
                    continue

                yield {
                    "_op_type": "update",
                    "_index": idx_name,
                    "_id": _id,
                    "doc": doc,
                    "doc_as_upsert": True,
                }

        os_helpers.bulk(
            self.client, gen_actions(), chunk_size=chunk_size, request_timeout=120
        )

    def _ensure_os_index(self, recreate: bool = False) -> None:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Å —Ä—É—Å—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º.
        OS_INDEX_ANSWER=true|false ‚Äî –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –ª–∏ –ø–æ–ª–µ answer –∫–∞–∫ text (–∏–Ω–∞—á–µ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –≤ _source).
        """
        idx_name = self.os_schema.index_name

        if recreate and self.client.indices.exists(
            index=idx_name, expand_wildcards="all"
        ):
            self.client.indices.delete(index=idx_name, ignore=[400, 404])

        if not self.client.indices.exists(index=idx_name):
            body = {
                "settings": self.os_schema.settings,
                "mappings": self.os_schema.mappings,
            }
            self.client.indices.create(index=idx_name, body=body)

    def _load_os_schema_from_json(self, path: str) -> OSIndexSchema:
        with open(path, encoding="utf-8") as f:
            data = json.load(f)

        # –ü–æ–ª—É—á–∞–µ–º id_field - –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —Å–ø–∏—Å–∫–æ–º
        id_field = data.get("id_field", "row_idx")

        # –ï—Å–ª–∏ id_field –∑–∞–¥–∞–Ω –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ "source,ext_id", –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
        if isinstance(id_field, str) and "," in id_field:
            id_field = [f.strip() for f in id_field.split(",")]

        return OSIndexSchema(
            index_name=data.get("index_name", self.config.index_name),
            id_field=id_field,  # –ú–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —Å–ø–∏—Å–∫–æ–º
            settings=data.get("settings", {}),
            mappings=data.get("mappings", {}),
            bulk_chunk_size=(data.get("bulk") or {}).get(
                "chunk_size", self.config.bulk_chunk_size
            ),
        )

    def fetch_existing(self, size: int = 10000) -> list[dict[str, tp.Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ OpenSearch"""
        idx_name = self.os_schema.index_name
        self.logger.info(f"üì• –ß—Ç–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ {idx_name} ...")
        try:
            query = {"query": {"match_all": {}}}
            resp = self.client.search(index=idx_name, body=query, size=size)
            hits = resp.get("hits", {}).get("hits", [])
            docs = [
                {
                    "ext_id": h["_id"].split("::")[1] if "::" in h["_id"] else h["_id"],
                    **h["_source"],
                }
                for h in hits
            ]
            self.logger.info(f"üìÑ –ü–æ–ª—É—á–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ OS")
            return docs
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ OS –∏–Ω–¥–µ–∫—Å–∞ {idx_name}: {e}")
            return []

    def delete(self, ext_ids: list[str]) -> None:
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ OpenSearch –ø–æ –ø–æ–ª—é ext_id."""
        idx_name = self.os_schema.index_name

        if not ext_ids:
            raise ValueError("–ù—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å ext_ids –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")

        self.logger.info(f"üßπ –£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {idx_name} –ø–æ ext_id ...")

        try:
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ –≤—Ä–µ–º—è bulk-–æ–ø–µ—Ä–∞—Ü–∏–π
            self._os_optimize_for_bulk(on=True)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å delete_by_query
            query = {"terms": {"ext_id": ext_ids}}

            resp = self.client.delete_by_query(
                index=idx_name,
                body={"query": query},
                refresh=True,
                conflicts="proceed",  # –Ω–µ –ø–∞–¥–∞–µ—Ç –Ω–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
                request_timeout=300,
            )
            deleted_count = resp.get("deleted", 0)
            self.logger.info(f"üóë –£–¥–∞–ª–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ ext_id: {deleted_count}")

        except Exception as e:
            self.logger.error(
                f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –∏–∑ OS –ø–æ ext_id ({type(e)}): {traceback.format_exc()}"
            )

        finally:
            self._os_optimize_for_bulk(on=False)

    def ensure_index_not_empty(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∏–Ω–¥–µ–∫—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ö–æ—Ç—è –±—ã 1 –¥–æ–∫—É–º–µ–Ω—Ç."""
        idx = self.os_schema.index_name

        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã bulk —Å—Ç–∞–ª –≤–∏–¥–∏–º—ã–º
            self.client.indices.refresh(index=idx)

            # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–¥—Å—á—ë—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            resp = self.client.count(index=idx)
            count = resp.get("count", 0)

            if count == 0:
                self.logger.error(f"‚ùå –ò–Ω–¥–µ–∫—Å {idx} –ø—É—Å—Ç –ø–æ—Å–ª–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏!")
                raise RuntimeError(f"–ò–Ω–¥–µ–∫—Å {idx} –ø—É—Å—Ç –ø–æ—Å–ª–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")

            self.logger.info(f"üì¶ –ò–Ω–¥–µ–∫—Å {idx} —Å–æ–¥–µ—Ä–∂–∏—Ç {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        except Exception as e:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ {idx}: {e}")

    def _resolve_keyword_field(self, idx_name: str, field: str) -> str | None:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–µ –¥–ª—è exact-—Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (terms): –ª–∏–±–æ —Å–∞–º field, –µ—Å–ª–∏ type=keyword,
        –ª–∏–±–æ field+'.keyword', –µ—Å–ª–∏ type=text –∏ –µ—Å—Ç—å –ø–æ–¥–ø–æ–ª–µ keyword,
        –∏–Ω–∞—á–µ None (–±—É–¥–µ–º –ø–∞–¥–∞—Ç—å –≤ —Ä–µ–∂–∏–º –ø–æ–ª–Ω–æ–≥–æ scan).
        """
        try:
            mapping = self.client.indices.get_mapping(index=idx_name)
            # –ï—Å–ª–∏ idx_name ‚Äî –∞–ª–∏–∞—Å, –±–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ —Ä–µ–∞–ª—å–Ω–æ–µ –∏–º—è –∏–Ω–¥–µ–∫—Å–∞
            m = next(iter(mapping.values()))  # {'mappings': {...}}
            props = (m.get("mappings") or {}).get("properties") or {}

            # –ï—Å–ª–∏ –ø–æ–ª–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ—á–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, "source.keyword")
            if "." in field:
                parts = field.split(".")
                current = props
                for part in parts[:-1]:
                    if part in current:
                        current = current[part].get("properties") or current[part]
                    else:
                        return None
                finfo = current.get(parts[-1], {})
            else:
                finfo = props.get(field, {})

            if finfo.get("type") == "keyword":
                return field

            # text —Å –ø–æ–¥–ø–æ–ª–µ–º keyword?
            sub = finfo.get("fields") or {}
            if "keyword" in sub and (sub["keyword"].get("type") == "keyword"):
                return f"{field}.keyword"

        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å mapping –¥–ª—è –ø–æ–ª—è '{field}': {e}")

        return None

    def ids_exist_by_source_field(
        self,
        incoming_ext_ids: tp.Iterable[tp.Any],
        source: str = None,
        field: str = "ext_id",
        batch_size: int = 2000,
        scan_page: int = 1000,
        scroll_keepalive: str = "5m",
    ) -> tuple[list[str], list[str], list[str]]:
        """–í–µ—Ä–Ω—ë—Ç —Ç—Ä–∏ —Å–ø–∏—Å–∫–∞ (—Å—Ç—Ä–æ–∫–∏):
        - found_incoming: –≤—Ö–æ–¥—è—â–∏–µ ext_id, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ –∏–Ω–¥–µ–∫—Å–µ –ø–æ _source[field]
        - missing_incoming: –≤—Ö–æ–¥—è—â–∏–µ ext_id, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∏–Ω–¥–µ–∫—Å–µ
        - extra_in_store: ext_id, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –∏–Ω–¥–µ–∫—Å–µ, –Ω–æ –∏—Ö –Ω–µ—Ç –≤–æ –≤—Ö–æ–¥—è—â–∏—Ö
        """
        idx_name = self.os_schema.index_name

        # –≤—Ö–æ–¥—è—â–∏–µ -> —Å—Ç—Ä–æ–∫–∏
        incoming_ids = [str(x) for x in incoming_ext_ids if x is not None]
        incoming_set = set(incoming_ids)

        # –ì–æ—Ç–æ–≤–∏–º –µ–¥–∏–Ω—ã–π term-—Ñ–∏–ª—å—Ç—Ä –ø–æ source (–µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω)
        source_filter: dict | None = None
        if source:
            source_term_field = self._resolve_keyword_field(idx_name, "source")
            source_field_for_term = source_term_field or "source"
            source_filter = {"term": {source_field_for_term: source}}

        term_field = self._resolve_keyword_field(idx_name, field)

        # –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è store_ids –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏
        store_ids: set[str] | None = None

        # 1) –Ω–∞—Ö–æ–¥–∏–º ¬´—á—Ç–æ –∏–∑ –≤—Ö–æ–¥—è—â–∏—Ö –µ—Å—Ç—å¬ª —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ source –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        found_incoming: set[str] = set()

        if term_field:
            # –±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å: —Ç–æ—á–Ω—ã–µ terms-–∑–∞–ø—Ä–æ—Å—ã –ø–æ keyword/–ø–æ–¥–ø–æ–ª—é
            base_filters = []
            if source_filter:
                base_filters.append(source_filter)

            for i in range(0, len(incoming_ids), batch_size):
                batch = incoming_ids[i : i + batch_size]

                must_clauses = [{"terms": {term_field: batch}}] + base_filters

                body = {
                    "_source": [field],
                    "query": {"bool": {"filter": must_clauses}},  # filter –≤–º–µ—Å—Ç–æ must
                    "size": len(batch),
                }
                resp = self.client.search(index=idx_name, body=body)

                for hit in resp.get("hits", {}).get("hits", []):
                    src = hit.get("_source") or {}
                    val = src.get(field)
                    if val is not None:
                        found_incoming.add(str(val))
        else:
            # –Ω–µ—Ç —Ç–æ—á–Ω–æ–≥–æ –ø–æ–ª—è ‚Äî –¥–µ–≥—Ä–∞–¥–∏—Ä—É–µ–º: —á–∏—Ç–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª—è –∏ –ø–µ—Ä–µ—Å–µ–∫–∞–µ–º
            self.logger.warning(
                f"–ü–æ–ª–µ '{field}' –Ω–µ keyword –∏ –±–µ–∑ –ø–æ–¥–ø–æ–ª–µ–π keyword ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º scan –¥–ª—è –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è."
            )
            store_ids_scan: set[str] = set()

            scan_query = (
                {"bool": {"filter": [source_filter]}}
                if source_filter
                else {"match_all": {}}
            )
            scan_body = {"_source": [field], "query": scan_query}

            for hit in os_helpers.scan(
                self.client,
                index=idx_name,
                query=scan_body,
                size=scan_page,
                scroll=scroll_keepalive,
            ):
                src = hit.get("_source") or {}
                v = src.get(field)
                if v is not None:
                    store_ids_scan.add(str(v))

            found_incoming = incoming_set & store_ids_scan
            store_ids = store_ids_scan  # –∑–∞–ø–æ–º–∏–Ω–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –≤—Ç–æ—Ä–æ–π —Ä–∞–∑

        missing_incoming = incoming_set - found_incoming

        # 2) extra_in_store ‚Äî –≤—Å—ë, —á—Ç–æ –≤ –∏–Ω–¥–µ–∫—Å–µ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º source, –Ω–æ –Ω–µ –ø—Ä–∏—à–ª–æ
        if store_ids is None:
            # –µ—â—ë –Ω–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–ª–∏ –∏–Ω–¥–µ–∫—Å
            store_ids = set()

            scan_query = (
                {"bool": {"filter": [source_filter]}}
                if source_filter
                else {"match_all": {}}
            )
            scan_body = {"_source": [field], "query": scan_query}

            for hit in os_helpers.scan(
                self.client,
                index=idx_name,
                query=scan_body,
                size=scan_page,
                scroll=scroll_keepalive,
            ):
                src = hit.get("_source") or {}
                v = src.get(field)
                if v is not None:
                    store_ids.add(str(v))

        extra_in_store = store_ids - incoming_set

        return (
            list(found_incoming),
            list(missing_incoming),
            list(extra_in_store),
        )

    def delete_by_ext_ids(
        self,
        ext_ids: list[str],
        field: str = "ext_id",
        batch_size: int = 2000,
        scan_page: int = 1000,
        scroll_keepalive: str = "5m",
    ) -> int:
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Å—Ç—Ä–æ–∫–æ–≤–æ–º—É –ø–æ–ª—é _source[field].
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        """
        idx_name = self.os_schema.index_name
        if not ext_ids:
            return 0

        # –ø—Ä–∏–≤–æ–¥–∏–º –≤—Ö–æ–¥ –∫ —Å—Ç—Ä–æ–∫–∞–º
        wanted = [str(x) for x in ext_ids if x is not None]
        wanted_set = set(wanted)

        deleted_total = 0
        term_field = self._resolve_keyword_field(idx_name, field)

        # –ù–∞ bulk-–æ–ø–µ—Ä–∞—Ü–∏–∏ —É—Å–∫–æ—Ä—è–µ–º –∏–Ω–¥–µ–∫—Å
        self._os_optimize_for_bulk(on=True)
        try:
            if term_field:
                # –ë—ã—Å—Ç—Ä—ã–π –∏ —Ç–æ—á–Ω—ã–π –ø—É—Ç—å: delete_by_query terms –±–∞—Ç—á–∞–º–∏
                for i in range(0, len(wanted), batch_size):
                    batch = wanted[i : i + batch_size]

                    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä
                    terms_dict: dict[str, list[str]] = {term_field: batch}

                    resp = self.client.delete_by_query(
                        index=idx_name,
                        body={"query": {"terms": terms_dict}},
                        refresh=False,  # –ù–ï –æ–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
                        conflicts="proceed",
                        request_timeout=300,
                    )
                    deleted_total += int(resp.get("deleted", 0))
            else:
                # –§–æ–ª–±—ç–∫: –Ω–∞–π–¥—ë–º _id –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ scan –∏ —É–¥–∞–ª–∏–º –ø–æ _id
                ids_to_delete: set[str] = set()

                scan_body = {"_source": [field], "query": {"match_all": {}}}

                for hit in os_helpers.scan(
                    self.client,
                    index=idx_name,
                    query=scan_body,
                    size=scan_page,
                    scroll=scroll_keepalive,
                ):
                    src = hit.get("_source") or {}
                    v = src.get(field)
                    if v is not None and str(v) in wanted_set:
                        ids_to_delete.add(hit["_id"])

                if ids_to_delete:

                    def gen_actions() -> tp.Generator[dict[str, str | int], None, None]:
                        for _id in ids_to_delete:
                            yield {"_op_type": "delete", "_index": idx_name, "_id": _id}

                    success, _ = os_helpers.bulk(
                        self.client,
                        gen_actions(),
                        chunk_size=batch_size,
                        request_timeout=300,
                    )
                    deleted_total += int(success)
        except Exception as e:
            self.logger.error(
                f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –∏–∑ OS –ø–æ ext_id ({type(e)}): {traceback.format_exc()}"
            )
        finally:
            # –î–µ–ª–∞–µ–º refresh –æ–¥–∏–Ω —Ä–∞–∑ –≤ –∫–æ–Ω—Ü–µ
            try:
                self.client.indices.refresh(index=idx_name)
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ refresh –∏–Ω–¥–µ–∫—Å–∞: {e}")
            self._os_optimize_for_bulk(on=False)

        self.logger.info(f"üóë OS: —É–¥–∞–ª–µ–Ω–æ {deleted_total} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ {field}")
        return deleted_total

    def diff_modified_by_ext_ids(
        self,
        incoming_modified: dict[str, str],
        *,
        field: str = "ext_id",
        modified_field: str = "modified_at",
        batch_size: int = 2000,
        scan_page: int = 1000,
        scroll_keepalive: str = "5m",
    ) -> list[str]:
        """–í–µ—Ä–Ω—ë—Ç —Å–ø–∏—Å–æ–∫ ext_id, —É –∫–æ—Ç–æ—Ä—ã—Ö modified_at –≤ –∏–Ω–¥–µ–∫—Å–µ OpenSearch –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è
        –æ—Ç –≤—Ö–æ–¥—è—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.
        """
        idx_name = self.os_schema.index_name

        # –≤—Ö–æ–¥—è—â–∏–µ -> —Å—Ç—Ä–æ–∫–∏
        incoming_map = {
            str(k): ("" if v is None else str(v)) for k, v in incoming_modified.items()
        }
        wanted_ids = list(incoming_map.keys())
        incoming_set = set(wanted_ids)

        diffs: set[str] = set()
        term_field = self._resolve_keyword_field(idx_name, field)

        if term_field:
            # –±—ã—Å—Ç—Ä—ã–π —Ç–æ—á–Ω—ã–π –ø—É—Ç—å
            for i in range(0, len(wanted_ids), batch_size):
                batch = wanted_ids[i : i + batch_size]
                if not batch:
                    continue

                # –ü—Ä–æ—Å—Ç–æ–π terms –∑–∞–ø—Ä–æ—Å –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ source
                terms_dict: dict[str, list[str]] = {term_field: batch}
                body = {
                    "_source": [field, modified_field],
                    "query": {"terms": terms_dict},
                    "size": len(batch),
                }
                resp = self.client.search(index=idx_name, body=body)
                for hit in resp.get("hits", {}).get("hits", []):
                    src = hit.get("_source") or {}
                    ext_val = src.get(field)
                    if ext_val is None:
                        continue
                    ext = str(ext_val)
                    idx_mod = (
                        ""
                        if src.get(modified_field) is None
                        else str(src.get(modified_field))
                    )
                    inc_mod = incoming_map.get(ext)
                    if inc_mod is None:
                        continue
                    if idx_mod != inc_mod:
                        diffs.add(ext)
        else:
            # —Ñ–æ–ª–±—ç–∫: —Å–∫–∞–Ω–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å, –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –Ω–∞ –ª–µ—Ç—É
            scan_body = {"_source": [field, modified_field], "query": {"match_all": {}}}
            for hit in os_helpers.scan(
                self.client,
                index=idx_name,
                query=scan_body,
                size=scan_page,
                scroll=scroll_keepalive,
            ):
                src = hit.get("_source") or {}
                ext_val = src.get(field)
                if ext_val is None:
                    continue
                ext = str(ext_val)
                if ext not in incoming_set:
                    continue
                idx_mod = (
                    ""
                    if src.get(modified_field) is None
                    else str(src.get(modified_field))
                )
                inc_mod = incoming_map[ext]
                if idx_mod != inc_mod:
                    diffs.add(ext)

        return list(diffs)
