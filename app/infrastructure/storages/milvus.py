import contextlib
import time
import traceback
import typing as tp

import numpy as np
from pymilvus import (
    AsyncMilvusClient,
    Collection,
    CollectionSchema,
    DataType,
    MilvusException,
)
from sentence_transformers import SentenceTransformer

from app.common.logger import AISearchLogger
from app.infrastructure.storages.interfaces import IVectorDatabase
from app.infrastructure.utils.metrics import metrics_print
from app.infrastructure.utils.milvus import load_schema_and_indexes_from_json
from app.infrastructure.utils.nlp import l2_normalize
from app.infrastructure.utils.universal import async_retry
from app.settings.config import MilvusSettings


class MilvusDatabase(IVectorDatabase):
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Milvus DB —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AsyncMilvusClient."""

    def __init__(self, settings: MilvusSettings, logger: AISearchLogger):
        milvus_init_start = time.perf_counter()
        self.config = settings
        self.logger = logger
        self.client = AsyncMilvusClient(
            uri=f"http{'s' if self.config.use_ssl else ''}://{self.config.host}:{self.config.port}",
            timeout=self.config.connection_timeout,
        )
        self.__collections_loaded = set()
        self._search_params_by_field = {}

        # –¢–æ–ª—å–∫–æ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π (metadata –ª–æ–≥–∏–∫–∞ —É–¥–∞–ª–µ–Ω–∞)

        metrics_print("üïí –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Milvus", milvus_init_start)

    @staticmethod
    def get_model_name(model: SentenceTransformer) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è –º–æ–¥–µ–ª–∏"""
        return model._first_module().auto_model.config._name_or_path.split("/")[-1]

    async def load_collection(self, collection_name: str) -> None:
        """–ü–æ–¥–≥—Ä—É–∑–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        await self.client.load_collection(
            collection_name, timeout=self.config.query_timeout
        )
        self.__collections_loaded.add(collection_name)

    async def create_collection(
        self,
        collection_name: str,
    ) -> None:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        collections = await self.client.list_collections(
            timeout=self.config.query_timeout
        )
        if collection_name in collections:
            await self.client.drop_collection(
                collection_name, timeout=self.config.query_timeout
            )

        fields, index_specs, search_params_by_field = load_schema_and_indexes_from_json(
            self.config.schema_path
        )

        schema = CollectionSchema(fields, description=f"Collection {collection_name}")
        await self.client.create_collection(
            collection_name=collection_name,
            schema=schema,
            timeout=self.config.query_timeout,
        )

        if index_specs:
            field_names = {f.name for f in fields}
            index_params = self.client.prepare_index_params()

            for idx in index_specs:
                if idx.field_name not in field_names:
                    raise ValueError(
                        f"–í JSON —É–∫–∞–∑–∞–Ω –∏–Ω–¥–µ–∫—Å –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –ø–æ–ª—è: {idx.field_name}"
                    )

                add_kwargs = {
                    "field_name": idx.field_name,
                    "index_type": idx.index_type,
                    "params": idx.params or {},
                }
                if idx.metric_type is not None:
                    add_kwargs["metric_type"] = idx.metric_type

                index_params.add_index(**add_kwargs)

            await self.client.create_index(
                collection_name=collection_name,
                index_params=index_params,
                timeout=self.config.query_timeout,
            )
        else:

            self.logger.warning(
                "üö® Milvus: –≤ JSON –Ω–µ –∑–∞–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ ‚Äî –∫–æ–ª–ª–µ–∫—Ü–∏—è –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ —Ä–µ–∂–∏–º–µ FLAT (–º–µ–¥–ª–µ–Ω–Ω–µ–µ)."
            )

        self._search_params_by_field = search_params_by_field or {}

        await self.load_collection(collection_name)

    async def insert_vectors(
        self,
        collection_name: str,
        vectors: list[list[float]],
        metadata: list[dict[str, tp.Any]] | None = None,
        batch_size: int = 512,
    ) -> None:
        """–í—Å—Ç–∞–≤–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ç–∏–ø–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏."""
        vectors_size = len(vectors)

        if metadata is not None:
            data_size = len(metadata)
            if vectors_size != data_size:
                raise ValueError(
                    f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–µ —Ä–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ({vectors_size} != {data_size})"
                )
        else:
            if vectors_size == 0:

                raise ValueError("–ù–µ–ª—å–∑—è –≤—Å—Ç–∞–≤–∏—Ç—å 0 –≤–µ–∫—Ç–æ—Ä–æ–≤ –±–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
            metadata = [{} for _ in range(vectors_size)]

        fields, index_specs, search_params_by_field = load_schema_and_indexes_from_json(
            self.config.schema_path
        )
        f_by_name = {f.name: f for f in fields}

        vec_field = self.config.vector_field
        if vec_field not in f_by_name:
            raise ValueError(f"–í —Å—Ö–µ–º–µ –Ω–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–ª—è '{vec_field}'")

        # dim –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç–æ–º .dim, —Ç–∞–∫ –∏ –≤ params (–≤ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö pymilvus)
        vfs = f_by_name[vec_field]
        dim = getattr(vfs, "dim", None)
        if dim is None:
            params = getattr(vfs, "params", {}) or {}
            dim = params.get("dim")

        # 2) –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ (—Ç–∏–ø –∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å)
        def _normalize_vector(vec: tp.Sequence[tp.Any]) -> list[float]:
            if dim is not None and len(vec) != dim:
                raise ValueError(f"–û–∂–∏–¥–∞–ª–∞—Å—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {len(vec)}, –ø–æ–ª—É—á–∏–ª–∏ {dim}")
            try:
                return [float(x) for x in vec]
            except Exception as e:
                raise TypeError(
                    f"–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –≤–µ–∫—Ç–æ—Ä–∞ –∫ float: {e}"
                ) from e

        def _coerce(name: str, value: tp.Any) -> tuple[tp.Any | None, bool]:
            """-> (coerced_value, drop)
            drop=True –æ–∑–Ω–∞—á–∞–µ—Ç '–Ω–µ –≤–∫–ª—é—á–∞—Ç—å —ç—Ç–æ –ø–æ–ª–µ –≤ –∑–∞–ø–∏—Å—å'
            """
            f = f_by_name.get(name)
            if f is None:
                return None, True
            if value is None:
                return None, True

            dt = f.dtype
            if dt == DataType.VARCHAR:
                s = str(value)
                max_len = getattr(f, "max_length", None)
                if max_len:
                    s = s[:max_len]
                return s, False
            if dt in (DataType.INT8, DataType.INT16, DataType.INT32, DataType.INT64):
                return int(value), False
            if dt in (DataType.FLOAT, DataType.DOUBLE):
                return float(value), False
            if dt == DataType.BOOL:
                return bool(value), False
            if dt == DataType.FLOAT_VECTOR:

                return None, True

            return value, False

        num_batches = (vectors_size + batch_size - 1) // batch_size
        self.logger.info(
            f"–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤{' –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö' if metadata else ''}, –±–∞—Ç—á {batch_size}, –≤—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {num_batches}"
        )

        for i in range(num_batches):
            start = i * batch_size
            end = min(start + batch_size, vectors_size)

            vectors_batch = vectors[start:end]
            rows_batch = metadata[start:end]

            # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ: {vector_field: [...], **coerced_meta}
            data: list[dict[str, tp.Any]] = []
            for vec, row in zip(vectors_batch, rows_batch, strict=True):
                item: dict[str, tp.Any] = {}

                # –≤–µ–∫—Ç–æ—Ä
                item[vec_field] = _normalize_vector(vec)

                # –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—Ç–æ–ª—å–∫–æ –ø–æ–ª—è –∏–∑ —Å—Ö–µ–º—ã, –±–µ–∑ None)
                for k, v in row.items():
                    val, drop = _coerce(k, v)
                    if not drop:
                        item[k] = val

                data.append(item)

            # –≤—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á–∞
            await self.client.insert(
                collection_name=collection_name,
                data=data,
                timeout=self.config.query_timeout,
            )
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {i + 1}/{num_batches} –±–∞—Ç—á–µ–π")

        # 5) –§–∏–Ω–∞–ª—å–Ω—ã–π flush (–æ–¥–∏–Ω —Ä–∞–∑, –ø–æ—Å–ª–µ –≤—Å–µ—Ö –±–∞—Ç—á–µ–π)
        await self.client.flush(collection_name, timeout=self.config.query_timeout)

    async def search(
        self, collection_name: str, query_vector: list[float], top_k: int
    ) -> list[dict[str, tp.Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏."""
        top_k = max(top_k, 1)

        if collection_name not in self.__collections_loaded:
            await self.load_collection(collection_name)

        results = await self.client.search(
            collection_name=collection_name,
            data=[query_vector],
            anns_field=self.config.vector_field,
            params={"metric_type": self.config.metric_type, "params": {"ef": 64}},
            limit=top_k,
            output_fields=self.config.output_fields,
            timeout=self.config.query_timeout,
        )

        out: list[dict[str, tp.Any]] = []
        if not results:
            return out

        hits = results[0]
        for h in hits:
            fields = h.entity
            row = {k: fields.get(k, "") for k in self.config.output_fields}
            row["score_dense"] = float(h.distance)
            out.append(row)
        return out

    async def collection_ready(self, collection_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
        try:
            collections = await self.client.list_collections(
                timeout=self.config.query_timeout
            )
            if collection_name not in collections:
                return False

            indexes = await self.client.list_indexes(collection_name)
            return len(indexes) > 0
        except Exception:
            return False

    async def delete_collection(self, collection_name: str) -> None:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
        self.logger.info(f"–£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name} ...")

        collections = await self.client.list_collections(
            timeout=self.config.query_timeout
        )
        if collection_name not in collections:
            self.logger.info(
                f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è."
            )
            self.__collections_loaded.discard(collection_name)
            return

        await self.client.drop_collection(
            collection_name, timeout=self.config.query_timeout
        )
        self.__collections_loaded.discard(collection_name)
        self.logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞")

    @async_retry(max_attempts=5, delay=3, exceptions=(MilvusException,))
    async def safe_delete_collection(self, collection_name: str) -> None:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        await self.delete_collection(collection_name)

    async def preload_collections(self) -> None:
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ –ø–∞–º—è—Ç—å"""
        collection_name = self.config.collection_name
        try:
            self.logger.info(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name} ...")
            await self.load_collection(collection_name)
            self.logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            self.logger.warning(
                f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name}: {e}"
            )

    async def index_documents(
        self,
        collection_name: str,
        model: SentenceTransformer,
        documents: list[str],
        metadata: list[dict[str, tp.Any]] | None = None,
    ) -> None:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ vector_db."""
        self.logger.info("–ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Milvus ...")

        embeddings = await self.get_embeddings(model, documents)
        await self.create_collection(collection_name)

        await self.insert_vectors(
            collection_name=collection_name,
            vectors=embeddings.tolist(),
            metadata=metadata,
        )

    async def get_embeddings(
        self, model: SentenceTransformer, documents: list[str]
    ) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        embeddings = model.encode(documents, normalize_embeddings=True)
        embeddings = np.vstack([l2_normalize(e) for e in embeddings])
        return embeddings

    async def initialize_collection(
        self,
        collection_name: str,
        model: SentenceTransformer,
        documents: list[str],
        metadata: list[dict[str, tp.Any]] | None = None,
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª—å—é."""
        self.logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name} ...")
        await self.index_documents(
            collection_name=collection_name,
            model=model,
            documents=documents,
            metadata=metadata,
        )

    async def close(self) -> None:
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –∫–ª–∏–µ–Ω—Ç–æ–º."""
        if hasattr(self, "client"):
            await self.client.close()

    async def fetch_existing(
        self, collection_name: str, output_fields: list[str] | None = None
    ) -> list[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ 8_192, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –¥–ª—è auto_id –∏ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–æ–ª–µ–π"""
        try:
            if collection_name not in self.__collections_loaded:
                await self.load_collection(collection_name)

            output_fields = (
                output_fields or self.config.output_fields.split(",")
                if isinstance(self.config.output_fields, str)
                else self.config.output_fields
            )
            # row_count = int(
            #     (await self.client.get_collection_stats(collection_name))["row_count"]
            # )
            results = []

            batch_size = 8_192
            last_pk = -1

            while True:
                # –ë–µ—Ä—ë–º –ø–∞–∫–µ—Ç –∑–∞–ø–∏—Å–µ–π –ø–æ –∞–≤—Ç–æ–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–Ω–æ–º—É PK
                filter_expr = f"pk > {last_pk}"
                batch_res = await self.client.query(
                    collection_name=collection_name,
                    filter=filter_expr,
                    output_fields=output_fields,
                    limit=batch_size,
                    timeout=self.config.query_timeout,
                )
                if not batch_res:
                    break
                results.extend(batch_res)
                last_pk = max(r["pk"] for r in batch_res)

            self.logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(results)} –∑–∞–ø–∏—Å–µ–π –∏–∑ Milvus")
            return results

        except Exception as e:
            self.logger.error(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∑–∞–ø–∏—Å–∏ –º–∏–ª—å–≤—É—Å {collection_name} ({type(e)}): {traceback.format_exc()}"
            )
            return []

    async def upsert_vectors(
        self,
        collection_name: str,
        vectors: list[list[float]],
        metadata: list[dict[str, tp.Any]] | None = None,
        batch_size: int = 512,
    ) -> None:
        """–í—Å—Ç–∞–≤–∫–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (upsert –ø–æ ext_id)."""
        if not metadata:
            metadata = [{} for _ in vectors]
        elif len(metadata) != len(vectors):
            raise ValueError(
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"
            )

        # 1) –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –ø–æ ext_id
        existing = await self.fetch_existing(
            collection_name, output_fields=["ext_id", "pk"]
        )
        ext_id_to_pk = {r["ext_id"]: r["pk"] for r in existing}

        # 2) –û–ø—Ä–µ–¥–µ–ª—è–µ–º pk, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ)
        pk_to_delete = [
            ext_id_to_pk[m["ext_id"]] for m in metadata if m["ext_id"] in ext_id_to_pk
        ]
        if pk_to_delete:
            # –í Milvus –Ω–µ—Ç delete_by_ids –¥–ª—è auto_id, –∏—Å–ø–æ–ª—å–∑—É–µ–º filter
            filter_expr = "pk in [" + ",".join(map(str, pk_to_delete)) + "]"
            await self.client.delete(
                collection_name=collection_name, filter=filter_expr
            )
            await self.client.flush(collection_name)

        # 3) –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        fields, _, _ = load_schema_and_indexes_from_json(self.config.schema_path)
        f_by_name = {f.name: f for f in fields}
        vec_field = self.config.vector_field
        dim = getattr(
            f_by_name[vec_field], "dim", f_by_name[vec_field].params.get("dim")
        )

        def _normalize(vec: list[tp.Any]) -> list[float]:
            if len(vec) != dim:
                raise ValueError(f"–û–∂–∏–¥–∞–ª–∞—Å—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {len(vec)}, –ø–æ–ª—É—á–∏–ª–∏ {dim}")
            return [float(x) for x in vec]

        def _coerce(name: str, value: tp.Any) -> tuple[tp.Any | bool]:
            f = f_by_name.get(name)
            if f is None or value is None:
                return None, True
            if f.dtype == DataType.VARCHAR:
                s = str(value)
                if getattr(f, "max_length", None):
                    s = s[: f.max_length]
                return s, False
            if f.dtype in (
                DataType.INT64,
                DataType.INT32,
                DataType.INT16,
                DataType.INT8,
            ):
                return int(value), False
            if f.dtype in (DataType.FLOAT, DataType.DOUBLE):
                return float(value), False
            if f.dtype == DataType.BOOL:
                return bool(value), False
            return value, False

        # 4) –í—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á–∞–º–∏
        total = len(vectors)
        num_batches = (total + batch_size - 1) // batch_size
        self.logger.info(
            f"Upsert {total} –≤–µ–∫—Ç–æ—Ä–æ–≤/–º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –±–∞—Ç—á {batch_size}, –≤—Å–µ–≥–æ {num_batches} –±–∞—Ç—á–µ–π"
        )

        for i in range(num_batches):
            start = i * batch_size
            end = min(start + batch_size, total)
            vec_batch = [_normalize(v) for v in vectors[start:end]]
            meta_batch = metadata[start:end]

            data = []
            for vec, row in zip(vec_batch, meta_batch, strict=True):
                item = {vec_field: vec}
                for k, v in row.items():
                    val, drop = _coerce(k, v)
                    if not drop:
                        item[k] = val
                data.append(item)

            await self.client.insert(
                collection_name=collection_name,
                data=data,
                timeout=self.config.query_timeout,
            )
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {i + 1}/{num_batches} –±–∞—Ç—á–µ–π")

        await self.client.flush(collection_name, timeout=self.config.query_timeout)
        self.logger.info("Upsert –∑–∞–≤–µ—Ä—à–µ–Ω ‚úÖ")

    async def delete_vectors(
        self,
        collection_name: str,
        ext_ids: list[str] | None = None,
        filter_expr: str | None = None,
    ) -> None:
        """–£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Milvus.

        –ú–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å:
          - –ø–æ —Å–ø–∏—Å–∫—É ext_id (list[str])
          - –∏–ª–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–º—É —Ñ–∏–ª—å—Ç—Ä—É (filter_expr)
        """
        if not ext_ids and not filter_expr:
            raise ValueError("–ù—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –ª–∏–±–æ ext_ids, –ª–∏–±–æ filter_expr")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –µ—â—ë –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if collection_name not in self.__collections_loaded:
            await self.load_collection(collection_name)

        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ ext_id ‚Äî —Ñ–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ä–∞–∂–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞
        if ext_ids:
            quoted_ids = ",".join(f"'{x}'" for x in ext_ids)
            filter_expr = f"ext_id in [{quoted_ids}]"

        self.logger.info(
            f"üßπ –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –∏–∑ {collection_name} –ø–æ —Ñ–∏–ª—å—Ç—Ä—É: {filter_expr}"
        )

        try:
            await self.client.delete(
                collection_name=collection_name,
                filter=filter_expr,
                timeout=self.config.query_timeout,
            )
            await self.client.flush(collection_name, timeout=self.config.query_timeout)
            self.logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ ({collection_name})")
        except Exception as e:
            self.logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –∏–∑ {collection_name} ({type(e)}): {traceback.format_exc()}"
            )

    async def collection_not_empty(self, collection_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ö–æ—Ç—è –±—ã 1 –∑–∞–ø–∏—Å—å —á–µ—Ä–µ–∑ fetch_existing"""
        try:
            existing = await self.fetch_existing(collection_name, output_fields=["pk"])
            row_count = len(existing)
            self.logger.info(f"Milvus: —Å—Ç—Ä–æ–∫ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º = {row_count}")
            return row_count > 0
        except Exception as e:
            self.logger.warning(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name}: {e}"
            )
            return False

    def _ensure_varchar_field(self, col: Collection, field: str) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –∏–º–µ–µ—Ç —Ç–∏–ø VarChar."""
        for f in col.schema.fields:
            if f.name == field:
                if f.dtype != DataType.VARCHAR:
                    raise TypeError(
                        f"–ü–æ–ª–µ '{field}' –≤ Milvus –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–≤—ã–º (VarChar), –∞ —Å–µ–π—á–∞—Å: {f.dtype}"
                    )
                return
        raise ValueError(f"–í –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –Ω–µ—Ç –ø–æ–ª—è '{field}'")

    def _escape_str_for_expr(self, s: str) -> str:
        return s.replace("\\", "\\\\").replace('"', '\\"')

    async def find_existing_ext_ids(
        self,
        collection_name: str,
        incoming_ext_ids: tp.Iterable[tp.Any],
        field: str = "ext_id",
        batch_size: int = 1000,
        iterator_batch: int = 4096,
        source_field: str | None = None,
        source: str | None = None,
    ) -> tuple[list[str], list[str], list[str]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - found_incoming: –≤—Ö–æ–¥—è—â–∏–µ ext_id, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        - missing_incoming: –≤—Ö–æ–¥—è—â–∏–µ ext_id, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        - extra_in_store: ext_id –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤–æ –≤—Ö–æ–¥—è—â–∏—Ö

        –ï—Å–ª–∏ –∑–∞–¥–∞–Ω—ã source_field –∏ source, –≤—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è
        —Ç–æ–ª—å–∫–æ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º, —É –∫–æ—Ç–æ—Ä—ã—Ö source_field == source.
        """
        # ------------------------------------
        # 0. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–µ–º—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        # ------------------------------------
        schema = await self.client.describe_collection(collection_name)
        fields = {f["name"]: f for f in schema["fields"]}

        if field not in fields:
            raise RuntimeError(
                f"Field '{field}' not found in collection '{collection_name}'"
            )

        if fields[field]["type"] != DataType.VARCHAR:
            raise RuntimeError(
                f"Field '{field}' must be VarChar, got {fields[field]['type']}"
            )

        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—Ä—ã source_field / source
        if (source is None) != (source_field is None):
            raise ValueError(
                "Both 'source' and 'source_field' must be provided together or omitted"
            )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª—è source_field, –µ—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ source –≤–∫–ª—é—á–µ–Ω–∞
        if source is not None and source_field is not None:
            if source_field not in fields:
                raise RuntimeError(
                    f"Field '{source_field}' not found in collection '{collection_name}'"
                )

            if fields[source_field]["type"] != DataType.VARCHAR:
                raise RuntimeError(
                    f"Field '{source_field}' must be VarChar, got {fields[source_field]['type']}"
                )

        # ------------------------------------
        # 1. Load –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        # ------------------------------------
        with contextlib.suppress(Exception):
            await self.load_collection(collection_name)

        # ------------------------------------
        # 2. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤—Ö–æ–¥—è—â–∏—Ö –≤ —Å—Ç—Ä–æ–∫–∏
        # ------------------------------------
        incoming_ids = [str(x) for x in incoming_ext_ids if x is not None]
        incoming_set = set(incoming_ids)
        found_incoming: set[str] = set()

        # —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ source (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        escaped_source_value: str | None = None
        if source is not None:
            escaped_source_value = self._escape_str_for_expr(source)

        # ------------------------------------
        # 3. –ü–æ–∏—Å–∫ –≤—Ö–æ–¥—è—â–∏—Ö –±–∞—Ç—á–∞–º–∏
        # ------------------------------------
        for i in range(0, len(incoming_ids), batch_size):
            batch = incoming_ids[i : i + batch_size]

            list_literal = ",".join(f'"{self._escape_str_for_expr(s)}"' for s in batch)
            base_expr = f"{field} in [{list_literal}]"

            if escaped_source_value is not None and source_field is not None:
                # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –ø–æ ext_id, –∏ –ø–æ source_field
                filter_expr = (
                    f"({base_expr}) AND {source_field} == " f'"{escaped_source_value}"'
                )
            else:
                filter_expr = base_expr

            res = await self.client.query(
                collection_name=collection_name,
                filter=filter_expr,
                output_fields=[field],
                offset=0,
                limit=batch_size,
                timeout=30.0,
            )

            for r in res:
                v = r.get(field)
                if v is not None:
                    found_incoming.add(str(v))

        missing_incoming = incoming_set - found_incoming

        # ------------------------------------
        # 4. extra_in_store ‚Äî —á–µ—Ä–µ–∑ pagination (offset+limit)
        # ------------------------------------
        extra_in_store: set[str] = set()
        store_ids: set[str] = set()

        # —Ñ–∏–ª—å—Ç—Ä –ø–æ source_field, –µ—Å–ª–∏ –∑–∞–¥–∞–Ω
        if escaped_source_value is not None and source_field is not None:
            extras_filter_expr = f'{source_field} == "{escaped_source_value}"'
        else:
            extras_filter_expr = ""  # –≤—Å—ë

        offset = 0
        while True:
            res = await self.client.query(
                collection_name=collection_name,
                filter=extras_filter_expr,
                output_fields=[field],
                offset=offset,
                limit=iterator_batch,
                timeout=30.0,
            )

            if not res:
                break

            for r in res:
                v = r.get(field)
                if v is not None:
                    store_ids.add(str(v))

            offset += iterator_batch

        extra_in_store = store_ids - incoming_set

        return (
            list(found_incoming),
            list(missing_incoming),
            list(extra_in_store),
        )

    async def delete_by_ext_ids(
        self,
        collection_name: str,
        ext_ids: list[str],
        field: str = "ext_id",
        batch_size: int = 1000,
    ) -> int:
        """–£–¥–∞–ª—è–µ—Ç entities –ø–æ —Å—Ç—Ä–æ–∫–æ–≤–æ–º—É –ø–æ–ª—é (VarChar) –±–∞—Ç—á–∞–º–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö entities.
        """
        if not ext_ids:
            return 0

        # ----------------------------
        # 1. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ incoming ext_ids –∫ —Å—Ç—Ä–æ–∫–∞–º
        # ----------------------------
        wanted = [str(x) for x in ext_ids if x is not None]
        if not wanted:
            return 0

        deleted_total = 0

        # ----------------------------
        # 2. –ë–∞—Ç—á–µ–≤–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ
        # ----------------------------
        for i in range(0, len(wanted), batch_size):
            batch = wanted[i : i + batch_size]

            list_literal = ",".join(f'"{self._escape_str_for_expr(v)}"' for v in batch)
            filter_expr = f"{field} in [{list_literal}]"

            try:
                mr = await self.client.delete(
                    collection_name=collection_name,
                    filter=filter_expr,
                    timeout=30.0,
                )
                deleted_total += int(mr.get("delete_count", 0))

            except Exception as e:
                self.logger.error(
                    f"Milvus delete batch failed in collection "
                    f"'{collection_name}' ({type(e)}): {traceback.format_exc()}"
                )
                continue

        self.logger.info(
            f"üóë Milvus: —É–¥–∞–ª–µ–Ω–æ ~{deleted_total} entities –ø–æ –ø–æ–ª—é '{field}'"
        )
        return deleted_total

    async def diff_modified_by_ext_ids(
        self,
        collection_name: str,
        incoming_modified: dict[str, str],
        *,
        field: str = "ext_id",
        modified_field: str = "modified_at",
        batch_size: int = 1000,
    ) -> list[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ ext_id, —É –∫–æ—Ç–æ—Ä—ã—Ö modified_at –≤ Milvus –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è
        –æ—Ç –≤—Ö–æ–¥—è—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è. –û–±–∞ –ø–æ–ª—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å VarChar.
        """
        # ----------------------------
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–µ–º—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        # ----------------------------
        schema = await self.client.describe_collection(collection_name)
        fields = {f["name"]: f for f in schema["fields"]}

        if field not in fields:
            raise RuntimeError(
                f"Field '{field}' not found in collection '{collection_name}'"
            )
        if modified_field not in fields:
            raise RuntimeError(
                f"Field '{modified_field}' not found in collection '{collection_name}'"
            )
        if fields[field]["type"] != DataType.VARCHAR:
            raise RuntimeError(
                f"Field '{field}' must be VarChar, got {fields[field]['type']}"
            )
        if fields[modified_field]["type"] != DataType.VARCHAR:
            raise RuntimeError(f"Field '{modified_field}' must be VarChar")

        # ----------------------------
        # 2. Load –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        # ----------------------------
        with contextlib.suppress(Exception):
            await self.load_collection(collection_name)

        # ----------------------------
        # 3. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤—Ö–æ–¥—è—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        # ----------------------------
        incoming_map = {
            str(k): ("" if v is None else str(v)) for k, v in incoming_modified.items()
        }
        ids = list(incoming_map.keys())

        diffs: set[str] = set()

        # ----------------------------
        # 4. Batch-query —Å filter
        # ----------------------------
        for i in range(0, len(ids), batch_size):
            batch = ids[i : i + batch_size]

            list_literal = ",".join(f'"{self._escape_str_for_expr(x)}"' for x in batch)
            filter_expr = f"{field} in [{list_literal}]"

            rows = await self.client.query(
                collection_name=collection_name,
                filter=filter_expr,
                output_fields=[field, modified_field],
                timeout=30.0,
            )

            for r in rows:
                ext_val = r.get(field)
                if ext_val is None:
                    continue

                ext = str(ext_val)
                idx_mod = (
                    "" if r.get(modified_field) is None else str(r.get(modified_field))
                )
                inc_mod = incoming_map.get(ext)
                if inc_mod is None:
                    continue

                if idx_mod != inc_mod:
                    diffs.add(ext)

        return list(diffs)
