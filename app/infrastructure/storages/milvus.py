import typing as tp
import time

import numpy as np

from pymilvus import (
    AsyncMilvusClient,
    CollectionSchema, DataType,
)
from sentence_transformers import SentenceTransformer

from app.common.logger import AISearchLogger
from app.infrastructure.storages.interfaces import IVectorDatabase
from app.infrastructure.utils.milvus import load_schema_and_indexes_from_json
from app.infrastructure.utils.nlp import l2_normalize
from app.infrastructure.utils.metrics import metrics_print
from app.settings.config import MilvusSettings


class MilvusDatabase(IVectorDatabase):
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Milvus DB —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AsyncMilvusClient."""

    def __init__(self, settings: MilvusSettings, logger: AISearchLogger):
        milvus_init_start = time.perf_counter()
        self.config = settings
        self.logger = logger
        self.client = AsyncMilvusClient(
            uri=f"http{'s' if self.config.use_ssl else ''}://{self.config.host}:{self.config.port}",
            timeout=self.config.connection_timeout,
        )
        self.__collections_loaded = set()
        self._search_params_by_field = set()


        # –¢–æ–ª—å–∫–æ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π (metadata –ª–æ–≥–∏–∫–∞ —É–¥–∞–ª–µ–Ω–∞)

        metrics_print("üïí –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Milvus", milvus_init_start)

    @staticmethod
    def get_model_name(model: SentenceTransformer) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è –º–æ–¥–µ–ª–∏"""
        return model._first_module().auto_model.config._name_or_path.split("/")[-1]

    async def create_collection(
            self,
            collection_name: str,
    ) -> None:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        collections = await self.client.list_collections(timeout=self.config.query_timeout)
        if collection_name in collections:
            await self.client.drop_collection(collection_name, timeout=self.config.query_timeout)

        fields, index_specs, search_params_by_field = load_schema_and_indexes_from_json(self.config.schema_path)

        schema = CollectionSchema(fields, description=f"Collection {collection_name}")
        await self.client.create_collection(
            collection_name=collection_name,
            schema=schema,
            timeout=self.config.query_timeout,
        )

        if index_specs:
            field_names = {f.name for f in fields}
            index_params = self.client.prepare_index_params()

            for idx in index_specs:
                if idx.field_name not in field_names:
                    raise ValueError(f"–í JSON —É–∫–∞–∑–∞–Ω –∏–Ω–¥–µ–∫—Å –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –ø–æ–ª—è: {idx.field_name}")


                add_kwargs = {
                    "field_name": idx.field_name,
                    "index_type": idx.index_type,
                    "params": idx.params or {},
                }
                if idx.metric_type is not None:
                    add_kwargs["metric_type"] = idx.metric_type

                index_params.add_index(**add_kwargs)

            await self.client.create_index(
                collection_name=collection_name,
                index_params=index_params,
                timeout=self.config.query_timeout,
            )
        else:

            self.logger.warning(
            "üö® Milvus: –≤ JSON –Ω–µ –∑–∞–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ ‚Äî –∫–æ–ª–ª–µ–∫—Ü–∏—è –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ —Ä–µ–∂–∏–º–µ FLAT (–º–µ–¥–ª–µ–Ω–Ω–µ–µ)."
            )


        self._search_params_by_field = search_params_by_field or {}

        await self.client.load_collection(collection_name, timeout=self.config.query_timeout)
        self.__collections_loaded.add(collection_name)

    async def insert_vectors(
            self,
            collection_name: str,
            vectors: list[list[float]],
            metadata: list[dict[str, tp.Any]] | None = None,
            batch_size: int = 512,
    ) -> None:
        """–í—Å—Ç–∞–≤–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ç–∏–ø–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏."""
        vectors_size = len(vectors)

        if metadata is not None:
            data_size = len(metadata)
            if vectors_size != data_size:
                raise ValueError(
                    f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–µ —Ä–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ({vectors_size} != {data_size})"
                )
        else:
            if vectors_size == 0:

                raise ValueError("–ù–µ–ª—å–∑—è –≤—Å—Ç–∞–≤–∏—Ç—å 0 –≤–µ–∫—Ç–æ—Ä–æ–≤ –±–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
            metadata = [{} for _ in range(vectors_size)]

        fields, index_specs, search_params_by_field = load_schema_and_indexes_from_json(self.config.schema_path)
        f_by_name = {f.name: f for f in fields}

        vec_field = self.config.vector_field
        if vec_field not in f_by_name:
            raise ValueError(f"–í —Å—Ö–µ–º–µ –Ω–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–ª—è '{vec_field}'")

        # dim –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç–æ–º .dim, —Ç–∞–∫ –∏ –≤ params (–≤ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö pymilvus)
        vfs = f_by_name[vec_field]
        dim = getattr(vfs, "dim", None)
        if dim is None:
            params = getattr(vfs, "params", {}) or {}
            dim = params.get("dim")

        # 2) –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ (—Ç–∏–ø –∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å)
        def _normalize_vector(vec: tp.Sequence[tp.Any]) -> list[float]:
            if dim is not None and len(vec) != dim:
                raise ValueError(f"–û–∂–∏–¥–∞–ª–∞—Å—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {dim}, –ø–æ–ª—É—á–∏–ª–∏ {len(vec)}")
            try:
                return [float(x) for x in vec]
            except Exception as e:
                raise TypeError(f"–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –≤–µ–∫—Ç–æ—Ä–∞ –∫ float: {e}") from e


        def _coerce(name: str, value: tp.Any) -> tuple[tp.Any | None, bool]:
            """
            -> (coerced_value, drop)
            drop=True –æ–∑–Ω–∞—á–∞–µ—Ç '–Ω–µ –≤–∫–ª—é—á–∞—Ç—å —ç—Ç–æ –ø–æ–ª–µ –≤ –∑–∞–ø–∏—Å—å'
            """
            f = f_by_name.get(name)
            if f is None:
                return None, True
            if value is None:
                return None, True

            dt = f.dtype
            if dt == DataType.VARCHAR:
                s = str(value)
                max_len = getattr(f, "max_length", None)
                if max_len:
                    s = s[:max_len]
                return s, False
            if dt in (DataType.INT8, DataType.INT16, DataType.INT32, DataType.INT64):
                return int(value), False
            if dt in (DataType.FLOAT, DataType.DOUBLE):
                return float(value), False
            if dt == DataType.BOOL:
                return bool(value), False
            if dt == DataType.FLOAT_VECTOR:

                return None, True

            return value, False


        num_batches = (vectors_size + batch_size - 1) // batch_size
        self.logger.info(
            f"–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤{' –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö' if metadata else ''}, –±–∞—Ç—á {batch_size}, –≤—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {num_batches}"
        )

        for i in range(num_batches):
            start = i * batch_size
            end = min(start + batch_size, vectors_size)

            vectors_batch = vectors[start:end]
            rows_batch = metadata[start:end]  # type: ignore[index]

            # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ: {vector_field: [...], **coerced_meta}
            data: list[dict[str, tp.Any]] = []
            for vec, row in zip(vectors_batch, rows_batch):
                item: dict[str, tp.Any] = {}

                # –≤–µ–∫—Ç–æ—Ä
                item[vec_field] = _normalize_vector(vec)

                # –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—Ç–æ–ª—å–∫–æ –ø–æ–ª—è –∏–∑ —Å—Ö–µ–º—ã, –±–µ–∑ None)
                for k, v in row.items():
                    val, drop = _coerce(k, v)
                    if not drop:
                        item[k] = val

                data.append(item)

            # –≤—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á–∞
            await self.client.insert(
                collection_name=collection_name,
                data=data,
                timeout=self.config.query_timeout,
            )
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {i + 1}/{num_batches} –±–∞—Ç—á–µ–π")

        # 5) –§–∏–Ω–∞–ª—å–Ω—ã–π flush (–æ–¥–∏–Ω —Ä–∞–∑, –ø–æ—Å–ª–µ –≤—Å–µ—Ö –±–∞—Ç—á–µ–π)
        await self.client.flush(collection_name, timeout=self.config.query_timeout)

    async def search(
            self, collection_name: str, query_vector: list[float], top_k: int
    ) -> list[dict[str, tp.Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏."""
        top_k = max(top_k, 1)

        if collection_name not in self.__collections_loaded:
            await self.client.load_collection(collection_name, timeout=self.config.query_timeout)
            self.__collections_loaded.add(collection_name)

        results = await self.client.search(
            collection_name=collection_name,
            data=[query_vector],
            anns_field=self.config.vector_field,
            params={"metric_type": self.config.metric_type, "params": {"ef": 64}},
            limit=top_k,
            output_fields=self.config.output_fields,
            timeout=self.config.query_timeout,
        )

        out: list[dict[str, tp.Any]] = []
        if not results:
            return out

        hits = results[0]
        for h in hits:
            fields = h.entity
            row = {k: fields.get(k, "") for k in self.config.output_fields}
            row["score_dense"] = float(h.distance)
            out.append(row)
        return out

    async def collection_ready(self, collection_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
        try:
            collections = await self.client.list_collections(timeout=self.config.query_timeout)
            if collection_name not in collections:
                return False

            indexes = await self.client.list_indexes(collection_name)
            return len(indexes) > 0
        except Exception:
            return False

    async def delete_collection(self, collection_name: str) -> None:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
        self.logger.info(f"–£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name} ...")

        collections = await self.client.list_collections(timeout=self.config.query_timeout)
        if collection_name not in collections:
            self.logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
            self.__collections_loaded.discard(collection_name)
            return

        await self.client.drop_collection(collection_name, timeout=self.config.query_timeout)
        self.__collections_loaded.discard(collection_name)
        self.logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞")

    async def preload_collections(self) -> None:
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ –ø–∞–º—è—Ç—å"""
        collection_name = self.config.collection_name
        try:
            self.logger.info(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name} ...")
            await self.client.load_collection(
                collection_name, timeout=self.config.query_timeout
            )
            self.__collections_loaded.add(collection_name)
            self.logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name}: {e}")

    async def index_documents(
            self,
            collection_name: str,
            model: SentenceTransformer,
            documents: list[str],
            metadata: list[dict[str, tp.Any]] | None = None,
    ) -> None:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ vector_db."""
        self.logger.info("–ü–†–û–ò–°–•–û–î–ò–¢ –ò–ù–î–ï–ö–°–ê–¶–ò–Ø")
        embeddings = model.encode(documents, normalize_embeddings=True)
        embeddings = np.vstack([l2_normalize(e) for e in embeddings])

        await self.create_collection(
            collection_name
        )

        await self.insert_vectors(
            collection_name=collection_name, vectors=embeddings.tolist(), metadata=metadata
        )

    async def ensure_collection(
            self,
            collection_name: str,
            model: SentenceTransformer,
            documents: list[str] | None = None,
            metadata: list[dict[str, tp.Any]] | None = None,
            recreate: bool = False,
    ) -> None:
        """–ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""

        if recreate:
            self.logger.info(f"‚è≥ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name} (recreate=True) ...")
            if await self.collection_ready(collection_name):
                await self.delete_collection(collection_name=collection_name)
            if documents is None:
                raise ValueError("–î–ª—è recreate=True –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å documents")
            await self.initialize_collection(
                collection_name=collection_name, model=model, documents=documents, metadata=metadata
            )
            return

        if not await self.collection_ready(collection_name):
            self.logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞—ë–º ...")
            if documents is None:
                raise ValueError("–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –Ω—É–∂–Ω—ã documents")
            await self.initialize_collection(
                collection_name=collection_name, model=model, documents=documents, metadata=metadata
            )
            return

        if collection_name not in self.__collections_loaded:
            await self.client.load_collection(collection_name, timeout=self.config.query_timeout)
            self.__collections_loaded.add(collection_name)
            self.logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –ø–æ–¥–≥—Ä—É–∂–µ–Ω–∞ –≤ –ø–∞–º—è—Ç—å")

    async def initialize_collection(
            self,
            collection_name: str,
            model: SentenceTransformer,
            documents: list[str],
            metadata: list[dict[str, tp.Any]] | None = None,
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª—å—é."""
        self.logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name} ...")
        await self.index_documents(
            collection_name=collection_name, model=model, documents=documents, metadata=metadata
        )

    async def close(self) -> None:
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –∫–ª–∏–µ–Ω—Ç–æ–º."""
        if hasattr(self, "client"):
            await self.client.close()
