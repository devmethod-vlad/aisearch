import gc
import importlib
import io
import typing as tp
import numpy as np
import pandas as pd
import torch
import unicodedata
from sentence_transformers import SentenceTransformer

from app.common.logger import AISearchLogger
from app.infrastructure.adapters.interfaces import IOpenSearchAdapter, IEduAdapter
from app.infrastructure.storages.interfaces import IVectorDatabase
from app.infrastructure.utils.nlp import l2_normalize
from app.services.interfaces import IUpdaterService
from app.settings.config import Settings


class UpdaterService(IUpdaterService):

    FIELD_MAPPING = {
        "–ò—Å—Ç–æ—á–Ω–∏–∫": "source",
        "ID": "ext_id",
        "ID —Å—Ç—Ä–∞–Ω–∏—Ü—ã": "page_id",
        "–ê–∫—Ç—É–∞–ª—å–Ω–æ": "actual",
        "2 –ª–∏–Ω–∏—è": "second_line",
        "–†–æ–ª—å": "role",
        "–ü—Ä–æ–¥—É–∫—Ç": "product",
        "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ": "space",
        "–ö–æ–º–ø–æ–Ω–µ–Ω—Ç": "component",
        "–í–æ–ø—Ä–æ—Å (markdown)": "question_md",
        "–í–æ–ø—Ä–æ—Å (clean)": "question",
        "–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–∫–∏ (markdown)": "analysis_md",
        "–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–∫–∏ (clean)": "analysis",
        "–û—Ç–≤–µ—Ç (markdown)": "answer_md",
        "–û—Ç–≤–µ—Ç (clean)": "answer",
        "–î–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è": "for_user",
        "Jira": "jira",
        "–û–±–Ω–æ–≤–ª–µ–Ω–æ": "modified_at"
    }

    def __init__(self, settings: Settings, logger: AISearchLogger,
                 edu: IEduAdapter, milvus: IVectorDatabase, os: IOpenSearchAdapter):
        self.settings = settings
        self.logger = logger
        self.edu = edu
        self.milvus = milvus
        self.os_adapter = os
        self.collection_name = settings.milvus.collection_name
        self.model: tp.Optional[SentenceTransformer] = None

    async def _load_excel_from_edu(self, file_type: str) -> pd.DataFrame:
        if file_type == "vio":
            file_data: io.BytesIO = await self.edu.download_vio_base_file()
        elif file_type == "kb":
            file_data: io.BytesIO = await self.edu.download_kb_base_file()
        else:
            raise ValueError(f"Unknown file_type: {file_type}")
        df = pd.read_excel(file_data)
        self.logger.info(f"–§–∞–π–ª '{file_type}' –∑–∞–≥—Ä—É–∂–µ–Ω, {len(df)} —Å—Ç—Ä–æ–∫")
        return df

    def _prepare_metadata(self, df: pd.DataFrame, file_type: str) -> pd.DataFrame:
        df = df.copy()
        df.rename(columns=self.FIELD_MAPPING, inplace=True)
        df["ext_id"] = df["ext_id"].astype(str)
        df = df[df["answer"].astype(str).str.len() > 2]
        df["row_idx"] = range(len(df))
        if file_type == "vio":
            df["space"] = df["space"].astype(str).str.strip()

            df = df[
                df["space"].notna()
                & (df["space"].str.strip() != "")
                & (df["space"].str.lower() != "–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ")
                ]

        return df

    async def _fetch_existing_data(self) -> dict[str, dict]:
        """–°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ Milvus –∏ OS –ø–æ ext_id"""
        all_fields = list(UpdaterService.FIELD_MAPPING.values()) + [self.settings.milvus.vector_field, "row_idx"]
        milvus_raw = await self.milvus.fetch_existing(self.collection_name, output_fields=all_fields)
        milvus_data = {str(r["ext_id"]): r for r in milvus_raw if r.get("ext_id")}
        os_raw = self.os_adapter.fetch_existing()
        os_data = {str(r["ext_id"]): r for r in os_raw if r.get("ext_id")}
        combined = milvus_data.copy()
        combined.update(os_data)  # OS –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
        return combined

    def normalize_text(self, val):
        """–ü—Ä–∏–≤–æ–¥–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ –∫ —Å—Ç—Ä–æ–∫–µ, —É–±–∏—Ä–∞–µ–º –Ω–µ–≤–∏–¥–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫."""
        if val is None:
            return None
        if isinstance(val, float) and np.isnan(val):
            return None
        if isinstance(val, str) and (val.strip() == "" or val.strip().lower() == "nan"):
            return None

        s = str(val).strip()
        s = s.replace("\xa0", " ")  # –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        s = s.replace("\r\n", "\n").replace("\r", "\n")  # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º CRLF –∏ CR
        # –ó–∞–º–µ–Ω—è–µ–º –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã –Ω–∞ –æ–¥–∏–Ω
        s = "\n".join([line.strip() for line in s.splitlines() if line.strip() != ""])
        s = unicodedata.normalize("NFKC", s)
        return s

    def _diff_records(self, incoming_df: pd.DataFrame, existing_data: dict[str, dict]) -> pd.DataFrame:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∏–ª–∏ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏.
        –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–µ modified_at ‚Äî —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ–≥–æ.
        –ï—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ–µ ‚Äî —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤—Å–µ –ø–æ–ª—è.
        """
        to_update = []

        for _, row in incoming_df.iterrows():
            ext_id = str(row["ext_id"])
            existing_row = existing_data.get(ext_id)


            if not existing_row:
                to_update.append(row)
                continue

            val_incoming_mod = str(row.get("modified_at") or "").strip()
            val_existing_mod = str(existing_row.get("modified_at") or "").strip()

            # –ï—Å–ª–∏ –µ—Å—Ç—å modified_at, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ–≥–æ
            if val_incoming_mod and val_existing_mod:
                if val_incoming_mod != val_existing_mod:
                    self.logger.warning(f"üïì –ò–∑–º–µ–Ω–µ–Ω–æ {ext_id}: modified_at {val_existing_mod!r} -> {val_incoming_mod!r}")
                    to_update.append(row)
                continue

            # –ï—Å–ª–∏ modified_at –Ω–µ—Ç ‚Äî —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ
            for col in incoming_df.columns:
                if col in ("row_idx", "modified_at"):
                    continue

                val_incoming = self.normalize_text(row[col])
                val_existing = self.normalize_text(existing_row.get(col))

                if val_incoming != val_existing:
                    self.logger.warning(f"‚úèÔ∏è –ò–∑–º–µ–Ω–µ–Ω–æ {ext_id}: {col} ‚Äî {val_existing!r} -> {val_incoming!r}")
                    to_update.append(row)
                    break

        return pd.DataFrame(to_update)

    async def _update_collection_from_df(self, df: pd.DataFrame):
        self.logger.info("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ç–µ–∫—É—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ ...")
        existing_data = await self._fetch_existing_data()

        # --- 1Ô∏è‚É£ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö ---
        df_to_update = self._diff_records(df, existing_data)

        # --- 2Ô∏è‚É£ –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ ext_id –±–æ–ª—å—à–µ –Ω–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã ---
        incoming_ids = set(df["ext_id"].astype(str))
        existing_ids = set(existing_data.keys())
        to_delete_ids = existing_ids - incoming_ids

        # --- 3Ô∏è‚É£ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ---
        if to_delete_ids:
            self.logger.warning(f"üóë –ù–∞–π–¥–µ–Ω–æ {len(to_delete_ids)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
        if df_to_update.empty and not to_delete_ids:
            self.logger.info("‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö, –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö –∏–ª–∏ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π ‚Äî –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
            return

        # --- 4Ô∏è‚É£ Upsert –Ω–æ–≤—ã—Ö/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö ---
        if not df_to_update.empty:
            if self.model is None:
                self.model = SentenceTransformer(self.settings.milvus.model_name)

            documents = df_to_update[self.settings.milvus.search_fields].astype(str).tolist()
            metadata = df_to_update.to_dict(orient="records")

            self.logger.info(f"‚¨ÜÔ∏è –î–æ–±–∞–≤–ª—è–µ–º/–æ–±–Ω–æ–≤–ª—è–µ–º {len(df_to_update)} –∑–∞–ø–∏—Å–µ–π ...")

            embeddings = self.model.encode(documents, normalize_embeddings=True)
            embeddings = np.vstack([l2_normalize(e) for e in embeddings])
            await self.milvus.upsert_vectors(self.collection_name, embeddings.tolist(), metadata)

            self.os_adapter.upsert(metadata)

        # --- 5Ô∏è‚É£ –£–¥–∞–ª–µ–Ω–∏–µ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö ---
        if to_delete_ids:
            try:
                await self.milvus.delete_vectors(self.collection_name, list(to_delete_ids))
                self.os_adapter.delete(list(to_delete_ids))
                self.logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {len(to_delete_ids)} –∑–∞–ø–∏—Å–µ–π –∏–∑ Milvus –∏ OpenSearch")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π: {e}")

        self.logger.info("‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    async def update_vio_base(self):
        harvest = await self.edu.provoke_harvest_to_edu(harvest_type="vio")
        if harvest:
            df = await self._load_excel_from_edu("vio")
            df = self._prepare_metadata(df, "vio")
            await self._update_collection_from_df(df)
            await self.cleanup_resources()
        else:
            self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ edu")

    async def update_kb_base(self):
        harvest = await self.edu.provoke_harvest_to_edu(harvest_type="kb")
        if harvest:
            df = await self._load_excel_from_edu("kb")
            df = self._prepare_metadata(df, "kb")
            await self._update_collection_from_df(df)
            await self.cleanup_resources()
        else:
            self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ edu")

    async def cleanup_resources(self):
        self.logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ ...")
        importlib.invalidate_caches()
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        self.logger.info("–†–µ—Å—É—Ä—Å—ã –æ—á–∏—â–µ–Ω—ã ‚úÖ")