import io

import pandas as pd
from sentence_transformers import SentenceTransformer

from app.common.logger import AISearchLogger
from app.infrastructure.adapters.interfaces import IEduAdapter, IOpenSearchAdapter
from app.infrastructure.storages.interfaces import IVectorDatabase
from app.infrastructure.utils.prepare_dataframe import (
    dedup_by_question_any,
    prepare_dataframe,
    rename_dataframe
)
from app.infrastructure.utils.universal import cleanup_resources
from app.services.interfaces import IUpdaterService
from app.settings.config import Settings


class UpdaterService(IUpdaterService):
    FIELD_MAPPING = {
        "Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº": "source",
        "ID": "ext_id",
        "ID ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹": "page_id",
        "ÐÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾": "actual",
        "2 Ð»Ð¸Ð½Ð¸Ñ": "second_line",
        "Ð Ð¾Ð»ÑŒ": "role",
        "ÐŸÑ€Ð¾Ð´ÑƒÐºÑ‚": "product",
        "ÐŸÑ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð¾": "space",
        "ÐšÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚": "component",
        "Ð’Ð¾Ð¿Ñ€Ð¾Ñ (markdown)": "question_md",
        "Ð’Ð¾Ð¿Ñ€Ð¾Ñ (clean)": "question",
        "ÐÐ½Ð°Ð»Ð¸Ð· Ð¾ÑˆÐ¸Ð±ÐºÐ¸ (markdown)": "analysis_md",
        "ÐÐ½Ð°Ð»Ð¸Ð· Ð¾ÑˆÐ¸Ð±ÐºÐ¸ (clean)": "analysis",
        "ÐžÑ‚Ð²ÐµÑ‚ (markdown)": "answer_md",
        "ÐžÑ‚Ð²ÐµÑ‚ (clean)": "answer",
        "Ð”Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ": "for_user",
        "Jira": "jira",
        "ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾": "modified_at",
    }

    def __init__(
        self,
        settings: Settings,
        logger: AISearchLogger,
        edu: IEduAdapter,
        milvus: IVectorDatabase,
        os: IOpenSearchAdapter,
    ):
        self.settings = settings
        self.logger = logger
        self.edu = edu
        self.milvus = milvus
        self.os = os
        self.collection_name = settings.milvus.collection_name
        self.model: SentenceTransformer | None = None

    async def _load_excel_from_edu(self, file_type: str) -> pd.DataFrame:
        if file_type == "vio":
            file_data: io.BytesIO = await self.edu.download_vio_base_file()
        elif file_type == "kb":
            file_data: io.BytesIO = await self.edu.download_kb_base_file()
        else:
            raise ValueError(f"Unknown file_type: {file_type}")
        df = pd.read_excel(file_data)
        self.logger.info(f"Ð¤Ð°Ð¹Ð» '{file_type}' Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½, {len(df)} ÑÑ‚Ñ€Ð¾Ðº")
        return df

    def _prepare_metadata(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()

        _, _, df_prepared = prepare_dataframe(
            df=df,
            logger=self.logger
        )

        return df_prepared

    async def _update_collection_from_df(
        self, df: pd.DataFrame, target_source: str
    ) -> None:
        if df.empty:
            return

        current_source = target_source
        self.logger.info(f"ðŸ”„ ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°: {current_source}")

        incoming_ext_ids = df["ext_id"].astype(str).tolist()

        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð‘Ð”
        os_found, os_missing, os_extra = self.os.ids_exist_by_source_field(
            incoming_ext_ids, source=current_source
        )

        mil_found, mil_missing, mil_extra = await self.milvus.find_existing_ext_ids(
            self.collection_name,
            incoming_ext_ids,
            source_field="source",
            source=current_source,
        )

        incoming_set = set(incoming_ext_ids)

        # Ð”Ð»Ñ OpenSearch
        to_delete_os = list(set(os_extra))
        if to_delete_os:
            self.logger.warning(
                f"ðŸ—‘ OpenSearch: ÑƒÐ´Ð°Ð»ÑÐµÐ¼ {len(to_delete_os)} Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²..."
            )
            try:
                deleted_count = self.os.delete_by_ext_ids(to_delete_os)
                self.logger.info(f"âœ… OpenSearch: ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ {deleted_count}, ext_ids: {to_delete_os}")
            except Exception as e:
                self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð² OpenSearch: {e}")

        # Ð”Ð»Ñ Milvus
        to_delete_milvus = list(set(mil_extra))
        if to_delete_milvus:
            self.logger.warning(
                f"ðŸ—‘ Milvus: ÑƒÐ´Ð°Ð»ÑÐµÐ¼ {len(to_delete_milvus)} entities..."
            )
            try:
                deleted_count = await self.milvus.delete_by_ext_ids(
                    self.collection_name, to_delete_milvus
                )
                self.logger.info(f"âœ… Milvus: ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ ~{deleted_count}, ext_ids: {to_delete_milvus}")
            except Exception as e:
                self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð² Milvus: {e}")

        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÑ‚ÑŒ/Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑ‚ÑŒ Ð² OpenSearch
        new_in_os = set(os_missing)
        self.logger.info(
            f"ðŸ—‘ OpenSearch: Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(new_in_os)}, ext_ids: {new_in_os}"
        )
        update_candidates_os = set(os_found)

        # Ð”Ð»Ñ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð² OS Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ modified_at
        if update_candidates_os:
            self.logger.info("ðŸ”Ž OpenSearch: ÑÑ€Ð°Ð²Ð½Ð¸Ð²Ð°ÐµÐ¼ modified_at...")
            try:
                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñƒ modified_at Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
                incoming_modified_map_os = {
                    str(r["ext_id"]): (
                        ""
                        if r.get("modified_at") is None
                        else str(r.get("modified_at")).strip()
                    )
                    for r in df[
                        df["ext_id"].astype(str).isin(update_candidates_os)
                    ].to_dict(orient="records")
                }

                os_different = set(
                    self.os.diff_modified_by_ext_ids(incoming_modified_map_os)
                )
            except Exception as e:
                self.logger.error(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ diff_modified_by_ext_ids Ð² OS: {e}")
                os_different = set()
        else:
            os_different = set()
        self.logger.info(
            f"ðŸ—‘ OpenSearch: Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(os_different)}, ext_ids: {os_different}"
        )
        # Ð”Ð»Ñ Milvus Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ð¾
        new_in_milvus = set(mil_missing)
        self.logger.info(
            f"ðŸ—‘ Milvus: Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(new_in_milvus)}, ext_ids: {new_in_milvus}"
        )
        update_candidates_mil = set(mil_found)

        if update_candidates_mil:
            self.logger.info("ðŸ”Ž Milvus: ÑÑ€Ð°Ð²Ð½Ð¸Ð²Ð°ÐµÐ¼ modified_at...")
            try:
                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñƒ modified_at Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
                incoming_modified_map_mil = {
                    str(r["ext_id"]): (
                        ""
                        if r.get("modified_at") is None
                        else str(r.get("modified_at")).strip()
                    )
                    for r in df[
                        df["ext_id"].astype(str).isin(update_candidates_mil)
                    ].to_dict(orient="records")
                }

                mil_different = set(
                    await self.milvus.diff_modified_by_ext_ids(
                        self.collection_name, incoming_modified_map_mil
                    )
                )
            except Exception as e:
                self.logger.error(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ð¸ modified_at Ð² Milvus: {e}")
                mil_different = set()
        else:
            mil_different = set()
        self.logger.info(
            f"ðŸ—‘ Milvus: Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(mil_different)}, ext_ids: {mil_different}"
        )
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ/Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð² ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð‘Ð” Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾
        to_upsert_os = (new_in_os | os_different) & incoming_set
        to_upsert_mil = (new_in_milvus | mil_different) & incoming_set

        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð´Ð»Ñ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ (ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸)
        to_upsert_all = to_upsert_os | to_upsert_mil

        if not to_upsert_all:
            self.logger.info("âœ… ÐÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ñ… Ð¸Ð»Ð¸ Ð¸Ð·Ð¼ÐµÐ½Ñ‘Ð½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ upsert.")
            return

        df_to_upsert = df[df["ext_id"].astype(str).isin(to_upsert_all)].copy()
        if df_to_upsert.empty:
            return

        if self.model is None:
            self.model = SentenceTransformer(self.settings.milvus.model_name)

        docs = df_to_upsert[self.settings.milvus.search_fields].astype(str).tolist()
        metadata = df_to_upsert.to_dict(orient="records")

        self.logger.info(f"â¬†ï¸ ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ upsert Ð´Ð»Ñ {len(metadata)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹...")

        try:
            embeddings = await self.milvus.get_embeddings(self.model, docs)
        except Exception as e:
            self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²: {e}")
            raise

        # Upsert Ð² Milvus (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð² Milvus)
        mil_metadata = [m for m in metadata if str(m["ext_id"]) in to_upsert_mil]
        if mil_metadata:
            try:
                mil_indices = [
                    i
                    for i, m in enumerate(metadata)
                    if str(m["ext_id"]) in to_upsert_mil
                ]
                mil_embeddings = embeddings[mil_indices]

                await self.milvus.upsert_vectors(
                    self.collection_name, mil_embeddings.tolist(), mil_metadata
                )
                self.logger.info(
                    f"âœ… Milvus: upsert Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½ Ð´Ð»Ñ {len(mil_metadata)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
                )
            except Exception as e:
                self.logger.error(f"âŒ Milvus upsert failed: {e}")

        # Upsert Ð² OpenSearch (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð² OS)
        os_metadata = [m for m in metadata if str(m["ext_id"]) in to_upsert_os]
        if os_metadata:
            try:
                self.os.upsert(os_metadata)
                self.logger.info(
                    f"âœ… OpenSearch: upsert Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½ Ð´Ð»Ñ {len(os_metadata)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
                )
            except Exception as e:
                self.logger.error(f"âŒ OpenSearch upsert failed: {e}")

        self.logger.info("âœ… ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾")

    async def update_vio_base(self) -> None:
        df = await self._load_excel_from_edu("vio")
        df = self._prepare_metadata(df)
        await self._update_collection_from_df(df, target_source="Ð’Ð¸Ðž")
        cleanup_resources(self.logger)

    async def update_kb_base(self) -> None:
        df = await self._load_excel_from_edu("kb")
        df = self._prepare_metadata(df)
        await self._update_collection_from_df(df, target_source="Ð¢ÐŸ")
        cleanup_resources(self.logger)

    async def update_all(self) -> None:
        df_kb = await self._load_excel_from_edu("kb")
        df_kv = await self._load_excel_from_edu("vio")
        df_combined = pd.concat([df_kb, df_kv])

        df_renamed = rename_dataframe(df_combined)
        df_renamed = df_renamed.drop_duplicates(subset=['ext_id'], keep="last")
        df_deduped = dedup_by_question_any(df_renamed)
        df_deduped = self._prepare_metadata(df_deduped)

        df_kb = df_deduped[df_deduped["source"] == "Ð¢ÐŸ"].copy()
        df_kv = df_deduped[df_deduped["source"] == "Ð’Ð¸Ðž"].copy()

        await self._update_collection_from_df(df_kb, target_source="Ð¢ÐŸ")
        await self._update_collection_from_df(df_kv, target_source="Ð’Ð¸Ðž")

        cleanup_resources(self.logger)
