import io

import pandas as pd
from sentence_transformers import SentenceTransformer

from app.common.logger import AISearchLogger
from app.infrastructure.adapters.interfaces import IEduAdapter, IOpenSearchAdapter
from app.infrastructure.storages.interfaces import IVectorDatabase
from app.infrastructure.utils.prepare_dataframe import (
    combine_validated_sources,
    dedup_by_question_any,
    load_field_mapping,
    prepare_dataframe,
    rename_dataframe,
    split_by_source,
    validate_dataframe,
)
from app.infrastructure.utils.universal import cleanup_resources
from app.services.interfaces import IUpdaterService
from app.settings.config import Settings


class UpdaterService(IUpdaterService):
    def __init__(
        self,
        settings: Settings,
        logger: AISearchLogger,
        edu: IEduAdapter,
        milvus: IVectorDatabase,
        os: IOpenSearchAdapter,
    ):
        self.settings = settings
        self.logger = logger
        self.edu = edu
        self.milvus = milvus
        self.os = os
        self.collection_name = settings.milvus.collection_name
        self.model: SentenceTransformer | None = None
        self.field_mapping = load_field_mapping(settings.app.field_mapping_schema_path)
        self.id_column = settings.app.data_unique_id

    async def _load_excel_from_edu(self, file_type: str) -> pd.DataFrame:
        if file_type == "vio":
            file_data: io.BytesIO = await self.edu.download_vio_base_file()
        elif file_type == "kb":
            file_data: io.BytesIO = await self.edu.download_kb_base_file()
        else:
            raise ValueError(f"Unknown file_type: {file_type}")
        df = pd.read_excel(file_data)
        self.logger.info(f"Ð¤Ð°Ð¹Ð» '{file_type}' Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½, {len(df)} ÑÑ‚Ñ€Ð¾Ðº")
        return df

    async def _update_collection_from_df(
        self, df: pd.DataFrame, target_source: str
    ) -> None:
        if df.empty:
            return
        current_source = target_source
        self.logger.info(f"ðŸ”„ ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°: {current_source}")

        incoming_ids = df[self.id_column].astype(str).tolist()

        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð‘Ð”
        os_found, os_missing, os_extra = await self.os.ids_exist_by_source_field(
            incoming_ids, source=current_source
        )

        mil_found, mil_missing, mil_extra = await self.milvus.find_existing_ext_ids(
            self.collection_name,
            incoming_ids,
            source_field="source",
            source=current_source,
        )

        incoming_set = set(incoming_ids)

        # Ð”Ð»Ñ OpenSearch
        to_delete_os = list(set(os_extra))
        if to_delete_os:
            self.logger.warning(
                f"ðŸ—‘ OpenSearch: ÑƒÐ´Ð°Ð»ÑÐµÐ¼ {len(to_delete_os)} Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²..."
            )
            try:
                deleted_count = await self.os.delete_by_ext_ids(to_delete_os)
                self.logger.info(
                    f"âœ… OpenSearch: ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ {deleted_count}, ids: {to_delete_os}"
                )
            except Exception as e:
                self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð² OpenSearch: {e}")

        # Ð”Ð»Ñ Milvus
        to_delete_milvus = list(set(mil_extra))
        if to_delete_milvus:
            self.logger.warning(
                f"ðŸ—‘ Milvus: ÑƒÐ´Ð°Ð»ÑÐµÐ¼ {len(to_delete_milvus)} entities, ids: {to_delete_milvus}"
            )
            try:
                deleted_count = await self.milvus.delete_by_ext_ids(
                    self.collection_name, to_delete_milvus
                )
                self.logger.info(f"âœ… Milvus: ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ ~{deleted_count}")
            except Exception as e:
                self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð² Milvus: {e}")

        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÑ‚ÑŒ/Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑ‚ÑŒ Ð² OpenSearch
        new_in_os = set(os_missing)
        self.logger.info(
            f"ðŸ—‘ OpenSearch: Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(new_in_os)}, ids: {new_in_os}"
        )
        update_candidates_os = set(os_found)

        # Ð”Ð»Ñ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð² OS Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ modified_at
        if update_candidates_os:
            self.logger.info("ðŸ”Ž OpenSearch: ÑÑ€Ð°Ð²Ð½Ð¸Ð²Ð°ÐµÐ¼ modified_at...")
            try:
                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñƒ modified_at Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
                incoming_modified_map_os = {
                    str(r[self.id_column]): (
                        ""
                        if r.get("modified_at") is None
                        else str(r.get("modified_at")).strip()
                    )
                    for r in df[
                        df[self.id_column].astype(str).isin(update_candidates_os)
                    ].to_dict(orient="records")
                }

                os_different = set(
                    await self.os.diff_modified_by_ext_ids(incoming_modified_map_os)
                )
            except Exception as e:
                self.logger.error(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ diff_modified_by_ext_ids Ð² OS: {e}")
                os_different = set()
        else:
            os_different = set()
        self.logger.info(
            f"ðŸ—‘ OpenSearch: Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(os_different)}, ids: {os_different}"
        )
        # Ð”Ð»Ñ Milvus Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ð¾
        new_in_milvus = set(mil_missing)
        self.logger.info(
            f"ðŸ—‘ Milvus: Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(new_in_milvus)}, ids: {new_in_milvus}"
        )
        update_candidates_mil = set(mil_found)

        if update_candidates_mil:
            self.logger.info("ðŸ”Ž Milvus: ÑÑ€Ð°Ð²Ð½Ð¸Ð²Ð°ÐµÐ¼ modified_at...")
            try:
                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñƒ modified_at Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
                incoming_modified_map_mil = {
                    str(r[self.id_column]): (
                        ""
                        if r.get("modified_at") is None
                        else str(r.get("modified_at")).strip()
                    )
                    for r in df[
                        df[self.id_column].astype(str).isin(update_candidates_mil)
                    ].to_dict(orient="records")
                }

                mil_different = set(
                    await self.milvus.diff_modified_by_ext_ids(
                        self.collection_name, incoming_modified_map_mil
                    )
                )
            except Exception as e:
                self.logger.error(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ð¸ modified_at Ð² Milvus: {e}")
                mil_different = set()
        else:
            mil_different = set()
        self.logger.info(
            f"ðŸ—‘ Milvus: Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(mil_different)}, ids: {mil_different}"
        )
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ/Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð² ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð‘Ð” Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾
        to_upsert_os = (new_in_os | os_different) & incoming_set
        to_upsert_mil = (new_in_milvus | mil_different) & incoming_set

        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð´Ð»Ñ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ (ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸)
        to_upsert_all = to_upsert_os | to_upsert_mil

        if not to_upsert_all:
            self.logger.info("âœ… ÐÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ñ… Ð¸Ð»Ð¸ Ð¸Ð·Ð¼ÐµÐ½Ñ‘Ð½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ upsert.")
            return

        df_to_upsert = df[df[self.id_column].astype(str).isin(to_upsert_all)].copy()
        if df_to_upsert.empty:
            return

        if self.model is None:
            self.model = SentenceTransformer(
                self.settings.milvus.model_name, local_files_only=True
            )

        docs = df_to_upsert[self.settings.milvus.search_fields].astype(str).tolist()
        metadata = df_to_upsert.to_dict(orient="records")

        self.logger.info(f"â¬†ï¸ ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ upsert Ð´Ð»Ñ {len(metadata)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹...")

        try:
            embeddings = await self.milvus.get_embeddings(self.model, docs)
        except Exception as e:
            self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²: {e}")
            raise

        # Upsert Ð² Milvus (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð² Milvus)
        mil_metadata = [m for m in metadata if str(m[self.id_column]) in to_upsert_mil]
        if mil_metadata:
            try:
                mil_indices = [
                    i
                    for i, m in enumerate(metadata)
                    if str(m[self.id_column]) in to_upsert_mil
                ]
                mil_embeddings = embeddings[mil_indices]

                await self.milvus.upsert_vectors(
                    self.collection_name, mil_embeddings.tolist(), mil_metadata
                )
                self.logger.info(
                    f"âœ… Milvus: upsert Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½ Ð´Ð»Ñ {len(mil_metadata)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
                )
            except Exception as e:
                self.logger.error(f"âŒ Milvus upsert failed: {e}")

        # Upsert Ð² OpenSearch (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð² OS)
        os_metadata = [m for m in metadata if str(m[self.id_column]) in to_upsert_os]
        if os_metadata:
            try:
                await self.os.upsert(os_metadata)
                self.logger.info(
                    f"âœ… OpenSearch: upsert Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½ Ð´Ð»Ñ {len(os_metadata)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
                )
            except Exception as e:
                self.logger.error(f"âŒ OpenSearch upsert failed: {e}")

        self.logger.info("âœ… ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾")

    async def update_vio_base(self) -> None:
        vio_df = await self._load_excel_from_edu("vio")
        vio_df_renamed = rename_dataframe(
            vio_df, self.settings.app.field_mapping_schema_path
        )
        vio_df_validated = validate_dataframe(
            vio_df_renamed,
            self.settings.app.field_mapping_schema_path,
            id_column=self.id_column,
        )
        _, _, df_prepared = prepare_dataframe(
            vio_df_validated, id_column=self.id_column
        )
        await self._update_collection_from_df(df_prepared, target_source="Ð’Ð¸Ðž")

        cleanup_resources(self.logger)

    async def update_kb_base(self) -> None:
        tp_df = await self._load_excel_from_edu("kb")
        tp_df_renamed = rename_dataframe(
            tp_df, self.settings.app.field_mapping_schema_path
        )
        tp_df_validated = validate_dataframe(
            tp_df_renamed,
            self.settings.app.field_mapping_schema_path,
            id_column=self.id_column,
        )
        _, _, df_prepared = prepare_dataframe(tp_df_validated, id_column=self.id_column)
        await self._update_collection_from_df(df_prepared, target_source="Ð¢ÐŸ")

        cleanup_resources(self.logger)

    async def update_all(self) -> None:
        df_kb = await self._load_excel_from_edu("kb")
        df_vio = await self._load_excel_from_edu("vio")
        df_kb_renamed = rename_dataframe(
            df_kb, self.settings.app.field_mapping_schema_path
        )
        df_vio_renamed = rename_dataframe(
            df_vio, self.settings.app.field_mapping_schema_path
        )

        df_kb_validated = validate_dataframe(
            df_kb_renamed,
            self.settings.app.field_mapping_schema_path,
            id_column=self.id_column,
        )
        df_vio_validated = validate_dataframe(
            df_vio_renamed,
            self.settings.app.field_mapping_schema_path,
            id_column=self.id_column,
        )

        combined_df = combine_validated_sources(
            [df_kb_validated, df_vio_validated], id_column=self.id_column
        )
        combined_df = dedup_by_question_any(combined_df)

        tp_df, vio_df = split_by_source(combined_df)

        _, _, df_prepared = prepare_dataframe(tp_df, id_column=self.id_column)
        await self._update_collection_from_df(df_prepared, target_source="Ð¢ÐŸ")

        _, _, df_prepared = prepare_dataframe(vio_df, id_column=self.id_column)
        await self._update_collection_from_df(df_prepared, target_source="Ð’Ð¸Ðž")

        cleanup_resources(self.logger)
