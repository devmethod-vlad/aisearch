import asyncio
import contextlib
import json
import typing as tp

from sentence_transformers import SentenceTransformer, CrossEncoder

from app.api.v1.dto.responses.hybrid_search import SearchResult
from app.common.storages.interfaces import KeyValueStorageProtocol
from app.infrastructure.adapters.interfaces import (
    IBM25Adapter,
    ICrossEncoderAdapter,
    ILLMQueue,
    IOpenSearchAdapter,
    IRedisSemaphore,
)
from app.infrastructure.storages.interfaces import IVectorDatabase
from app.infrastructure.utils.nlp import hash_query, normalize_query
from app.services.interfaces import IHybridSearchOrchestrator
from app.settings.config import Settings


class HybridSearchOrchestrator(IHybridSearchOrchestrator):
    """ÐžÑ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€ Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°"""

    def __init__(
        self,
        queue: ILLMQueue,
        sem: IRedisSemaphore,
        vector_db: IVectorDatabase,
        bm25: IBM25Adapter,
        ce: ICrossEncoderAdapter,
        os_adapter: IOpenSearchAdapter,
        settings: Settings,
        redis: KeyValueStorageProtocol,
    ):
        self.queue = queue
        self.sem = sem
        self.vector_db = vector_db
        self.bm25 = bm25
        self.ce = ce
        self.os_adapter = os_adapter
        self.redis = redis
        self.settings = settings.hybrid
        self.switches = settings.switches
        self.use_cache = settings.app.use_cache

    # def initialize_orchestrator(self):
    #     print("Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€Ð°")

    async def documents_search(
        self,
        task_id: str,
        ticket_id: str,
        pack_key: str,
        result_key: str,
        model: SentenceTransformer,
        ce_model: CrossEncoder,
    ) -> dict[str, str]:
        """ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼"""
        import time
        start_total = time.perf_counter()

        await self.queue.set_running(ticket_id, task_id)

        # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ pack Ð¸Ð· Redis
        redis_get_start = time.perf_counter()
        raw = await self.redis.get(pack_key)
        redis_get_time = time.perf_counter() - redis_get_start
        print(f"â±ï¸  Redis GET pack: {redis_get_time:.4f} ÑÐµÐº")

        if not raw:
            await self.queue.set_failed(ticket_id, "missing pack")
            await self.queue.ack(ticket_id)
            return {"status": "error", "error": "missing pack"}

        # ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ JSON
        json_parse_start = time.perf_counter()
        pack = json.loads(raw)
        json_parse_time = time.perf_counter() - json_parse_start
        print(f"â±ï¸  JSON parse: {json_parse_time:.4f} ÑÐµÐº")
        normalize_start_time = time.perf_counter()
        query = normalize_query(pack["query"])
        query_hash = hash_query(query)
        top_k = int(pack["top_k"])
        need_ack = True
        normalize_end_time = time.perf_counter()
        norm_time = normalize_end_time - normalize_start_time
        print(f"â±ï¸  NORMALIZE_TIME: {norm_time:.4f} ÑÐµÐº")
        try:
            # ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ ÑÐµÐ¼Ð°Ñ„Ð¾Ñ€Ð°
            sem_acquire_start = time.perf_counter()
            async with self.sem.acquire():
                sem_acquire_time = time.perf_counter() - sem_acquire_start
                print(f"â±ï¸  Semaphore acquire: {sem_acquire_time:.4f} ÑÐµÐº")

                # Ð’ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
                encode_start = time.perf_counter()
                query_vector = (await asyncio.to_thread(model.encode, [query], convert_to_numpy=True))[0]
                encode_time = time.perf_counter() - encode_start
                print(f"â±ï¸  Model encode: {encode_time:.4f} ÑÐµÐº")
                cache_key = f"hyb:{query_hash}:{top_k}:{self.settings.version}"
                if self.use_cache:
                    cache_get_start = time.perf_counter()
                    cached = await self.redis.get(cache_key)
                    cache_get_time = time.perf_counter() - cache_get_start
                    print(f"â±ï¸  Cache GET: {cache_get_time:.4f} ÑÐµÐº")
                else:
                    cached = None
                if cached:
                    print("âœ… Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐºÐµÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚")
                    cache_parse_start = time.perf_counter()
                    results = [SearchResult(**x) for x in json.loads(cached)]
                    cache_parse_time = time.perf_counter() - cache_parse_start
                    print(f"â±ï¸  Cache parse: {cache_parse_time:.4f} ÑÐµÐº")
                else:
                    print("ðŸ” Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº")
                    # Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº
                    vector_search_start = time.perf_counter()
                    dense = await self.vector_db.search(
                        collection_name=self.settings.collection_name,
                        query_vector=query_vector,
                        top_k=top_k,
                    )
                    vector_search_time = time.perf_counter() - vector_search_start
                    print(f"â±ï¸  Vector search: {vector_search_time:.4f} ÑÐµÐº")

                    lex = []
                    lex_search_time = 0

                    # Ð›ÐµÐºÑÐ¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº
                    if self.switches.use_hybrid:
                        lex_search_start = time.perf_counter()
                        if self.switches.use_opensearch:
                            lex = await self._os_candidates(query, self.settings.lex_top_k)
                            print(f"ðŸ” OpenSearch candidates: {len(lex)} Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²")
                        elif self.switches.use_bm25:
                            lex = await self._bm25_candidates(query, self.settings.lex_top_k)
                            print(f"ðŸ” BM25 candidates: {len(lex)} Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²")
                        lex_search_time = time.perf_counter() - lex_search_start
                        print(f"â±ï¸  Lexical search: {lex_search_time:.4f} ÑÐµÐº")

                    # Ð¡Ð»Ð¸ÑÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
                    merge_start = time.perf_counter()
                    merged = self._merge_candidates(dense, lex)
                    merge_time = time.perf_counter() - merge_start
                    print(f"â±ï¸  Merge candidates: {merge_time:.4f} ÑÐµÐº")
                    print(f"ðŸ“Š ÐŸÐ¾ÑÐ»Ðµ ÑÐ»Ð¸ÑÐ½Ð¸Ñ: {len(merged)} ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð²")

                    # Re-ranking
                    rerank_time = 0
                    if self.switches.use_reranker and merged:
                        print("ðŸŽ¯ ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ÑÑ re-ranking")
                        rerank_start = time.perf_counter()

                        pairs_prep_start = time.perf_counter()
                        pairs = [(query, self._concat_text(m)) for m in merged]
                        pairs_prep_time = time.perf_counter() - pairs_prep_start
                        print(f"â±ï¸  Pairs preparation: {pairs_prep_time:.4f} ÑÐµÐº")

                        ce_rank_start = time.perf_counter()
                        ce_scores = await asyncio.to_thread(self.ce.rank, ce_model, pairs)
                        ce_rank_time = time.perf_counter() - ce_rank_start
                        print(f"â±ï¸  Cross-encoder rank: {ce_rank_time:.4f} ÑÐµÐº")

                        for m, s in zip(merged, ce_scores):
                            m["score_ce"] = float(s)
                        rerank_time = time.perf_counter() - rerank_start
                        print(f"â±ï¸  Total re-ranking: {rerank_time:.4f} ÑÐµÐº")

                    # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ° Ð¸ ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ°
                    scoring_start = time.perf_counter()
                    _results = self._score_and_slice(
                        merged, top_k, use_ce=self.switches.use_reranker
                    )
                    scoring_time = time.perf_counter() - scoring_start
                    print(f"â±ï¸  Final scoring: {scoring_time:.4f} ÑÐµÐº")

                    results = [SearchResult(**r) for r in _results]

                    if self.use_cache:
                        cache_set_start = time.perf_counter()
                        await self.redis.set(
                            cache_key,
                            json.dumps([r.model_dump() for r in results]),
                            ttl=self.settings.cache_ttl,
                        )
                        cache_set_time = time.perf_counter() - cache_set_start
                        print(f"â±ï¸  Cache SET: {cache_set_time:.4f} ÑÐµÐº")

                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
                result_prep_start = time.perf_counter()
                payload = {"results": [r.model_dump() for r in results]}
                result_prep_time = time.perf_counter() - result_prep_start
                print(f"â±ï¸  Result preparation: {result_prep_time:.4f} ÑÐµÐº")

                result_set_start = time.perf_counter()
                await self.redis.set(
                    result_key, json.dumps(payload, ensure_ascii=False), ttl=self.queue.ticket_ttl
                )
                result_set_time = time.perf_counter() - result_set_start
                print(f"â±ï¸  Result SET: {result_set_time:.4f} ÑÐµÐº")

                queue_done_start = time.perf_counter()
                await self.queue.set_done(ticket_id)
                queue_done_time = time.perf_counter() - queue_done_start
                print(f"â±ï¸  Queue set done: {queue_done_time:.4f} ÑÐµÐº")

                total_time = time.perf_counter() - start_total
                print(f"âœ… ÐŸÐžÐ˜Ð¡Ðš Ð—ÐÐ’Ð•Ð Ð¨Ð•Ð: {total_time:.4f} ÑÐµÐº")
                print(f"ðŸ“Š Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹: {len(results)} Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²")

                return {"status": "ok"}

        except TimeoutError as e:
            timeout_time = time.perf_counter() - start_total
            print(f"â° TIMEOUT Ñ‡ÐµÑ€ÐµÐ· {timeout_time:.4f} ÑÐµÐº: {e}")
            await self.queue.requeue(ticket_id, reason="global semaphore busy")
            need_ack = False
            return {"status": "requeued"}
        except Exception as e:
            error_time = time.perf_counter() - start_total
            print(f"âŒ ÐžÐ¨Ð˜Ð‘ÐšÐ Ñ‡ÐµÑ€ÐµÐ· {error_time:.4f} ÑÐµÐº: {e}")
            await self.queue.set_failed(ticket_id, str(e))
            raise
        finally:
            if need_ack:
                ack_start = time.perf_counter()
                with contextlib.suppress(Exception):
                    await self.queue.ack(ticket_id)
                ack_time = time.perf_counter() - ack_start
                print(f"â±ï¸  Queue ACK: {ack_time:.4f} ÑÐµÐº")

            final_total = time.perf_counter() - start_total
            print(f"ðŸ ÐžÐ‘Ð©Ð•Ð• Ð’Ð Ð•ÐœÐ¯ Ð’Ð«ÐŸÐžÐ›ÐÐ•ÐÐ˜Ð¯: {final_total:.4f} ÑÐµÐº")
            print("=" * 60)

    async def _os_candidates(self, query: str, k: int) -> list[dict[str, tp.Any]]:
        os_adapter = self.os_adapter
        fields = ["question", "analysis", "answer"]
        body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": fields,
                    "operator": os_adapter.config.operator,
                    "fuzziness": os_adapter.config.fuzziness,
                }
            }
        }
        hits = await asyncio.to_thread(os_adapter.search, body, k)
        out = []
        for h in hits:
            src = h.get("_source", {})
            out.append(
                {
                    "ext_id": src.get("ext_id", h.get("_id")),
                    "question": src.get("question", ""),
                    "analysis": src.get("analysis", ""),
                    "answer": src.get("answer", ""),
                    "score_lex": float(h.get("_score", 0.0)),
                    "source": "opensearch",
                }
            )
        return out

    async def _bm25_candidates(self, query: str, k: int) -> list[dict[str, tp.Any]]:
        rows = await asyncio.to_thread(self.bm25.search, query, k)
        out = []
        for r in rows:
            out.append(
                {
                    "ext_id": r.get("ext_id"),
                    "question": r.get("question", ""),
                    "analysis": r.get("analysis", ""),
                    "answer": r.get("answer", ""),
                    "score_lex": float(r.get("score_bm25", 0.0)),
                    "source": "bm25",
                }
            )
        return out

    def _merge_candidates(
        self, dense: list[dict[str, tp.Any]], lex: list[dict[str, tp.Any]]
    ) -> list[dict[str, tp.Any]]:
        by_id: dict[str, dict[str, tp.Any]] = {}
        for d in dense:
            by_id[d["ext_id"]] = {**d}
        for l in lex:  # noqa: E741
            x = by_id.get(l["ext_id"]) or {}
            x.setdefault("ext_id", l["ext_id"])
            for f in ("question", "analysis", "answer"):
                if not x.get(f):
                    x[f] = l.get(f, "")
            prev = x.get("score_lex", 0.0)
            x["score_lex"] = max(prev, float(l.get("score_lex", 0.0)))
            x["source"] = l["source"]
            by_id[x["ext_id"]] = x
        return list(by_id.values())

    def _score_and_slice(
        self, items: list[dict[str, tp.Any]], top_k: int, *, use_ce: bool
    ) -> list[dict[str, tp.Any]]:
        if not items:
            return []

        def _max(key: str) -> float:
            return max((i.get(key, 0.0) for i in items), default=1.0) or 1.0

        m_dense = _max("score_dense")
        m_lex = _max("score_lex")
        m_ce = _max("score_ce") if use_ce else 1.0
        for it in items:
            nd = (it.get("score_dense", 0.0) / m_dense) if m_dense else 0.0
            nl = (it.get("score_lex", 0.0) / m_lex) if m_lex else 0.0
            nc = (it.get("score_ce", 0.0) / m_ce) if use_ce else 0.0
            it["score_final"] = (
                self.settings.w_dense * nd
                + self.settings.w_lex * nl
                + (self.settings.w_ce * nc if use_ce else 0.0)
            )
        items.sort(key=lambda x: x.get("score_final", 0.0), reverse=True)
        return items[:top_k]

    def _concat_text(self, m: dict[str, tp.Any]) -> str:
        return "\n".join([m.get("question", ""), m.get("analysis", ""), m.get("answer", "")])
